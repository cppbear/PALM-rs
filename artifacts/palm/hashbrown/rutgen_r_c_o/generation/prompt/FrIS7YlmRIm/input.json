{
  "system_pt": "As a software testing expert, infer the test input ranges based on the provided information. Follow these guidelines:\n1. Provide test input ranges in one line in plain text only, without additional explanations or Markdown formatting.\n2. The inferred test input ranges should only satisfy all provided constraints simultaneously.\n3. Ensure the test input ranges cover boundary cases and edge scenarios.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\npub(crate) struct RawIterRange<T> {\n    current_group: BitMaskIter,\n    data: Bucket<T>,\n    next_ctrl: *const u8,\n    end: *const u8,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\n#[derive(Copy, Clone)]\npub(crate) struct BitMaskIter(pub(crate) BitMask);\npub struct Bucket<T> {\n    ptr: NonNull<T>,\n}\n#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n#[repr(transparent)]\npub(crate) struct Tag(u8);\nimpl<T> RawIterRange<T> {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new(ctrl: *const u8, data: Bucket<T>, len: usize) -> Self {\n        debug_assert_ne!(len, 0);\n        debug_assert_eq!(ctrl as usize % Group::WIDTH, 0);\n        let end = ctrl.add(len);\n        let current_group = Group::load_aligned(ctrl.cast()).match_full();\n        let next_ctrl = ctrl.add(Group::WIDTH);\n        Self {\n            current_group: current_group.into_iter(),\n            data,\n            next_ctrl,\n            end,\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    #[cfg(feature = \"rayon\")]\n    pub(crate) fn split(mut self) -> (Self, Option<RawIterRange<T>>) {\n        unsafe {\n            if self.end <= self.next_ctrl {\n                (self, None)\n            } else {\n                let len = offset_from(self.end, self.next_ctrl);\n                debug_assert_eq!(len % Group::WIDTH, 0);\n                let mid = (len / 2) & !(Group::WIDTH - 1);\n                let tail = Self::new(\n                    self.next_ctrl.add(mid),\n                    self.data.next_n(Group::WIDTH).next_n(mid),\n                    len - mid,\n                );\n                debug_assert_eq!(\n                    self.data.next_n(Group::WIDTH).next_n(mid).ptr, tail.data.ptr\n                );\n                debug_assert_eq!(self.end, tail.end);\n                self.end = self.next_ctrl.add(mid);\n                debug_assert_eq!(self.end.add(Group::WIDTH), tail.next_ctrl);\n                (self, Some(tail))\n            }\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn next_impl<const DO_CHECK_PTR_RANGE: bool>(&mut self) -> Option<Bucket<T>> {\n        loop {\n            if let Some(index) = self.current_group.next() {\n                return Some(self.data.next_n(index));\n            }\n            if DO_CHECK_PTR_RANGE && self.next_ctrl >= self.end {\n                return None;\n            }\n            self.current_group = Group::load_aligned(self.next_ctrl.cast())\n                .match_full()\n                .into_iter();\n            self.data = self.data.next_n(Group::WIDTH);\n            self.next_ctrl = self.next_ctrl.add(Group::WIDTH);\n        }\n    }\n    #[allow(clippy::while_let_on_iterator)]\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn fold_impl<F, B>(mut self, mut n: usize, mut acc: B, mut f: F) -> B\n    where\n        F: FnMut(B, Bucket<T>) -> B,\n    {}\n}\nimpl IntoIterator for BitMask {\n    type Item = usize;\n    type IntoIter = BitMaskIter;\n    #[inline]\n    fn into_iter(self) -> BitMaskIter {\n        BitMaskIter(BitMask(self.0 & BITMASK_ITER_MASK))\n    }\n}\nimpl Iterator for BitMaskIter {\n    type Item = usize;\n    #[inline]\n    fn next(&mut self) -> Option<usize> {\n        let bit = self.0.lowest_set_bit()?;\n        self.0 = self.0.remove_lowest_bit();\n        Some(bit)\n    }\n}\nimpl<T> Bucket<T> {\n    #[inline]\n    unsafe fn from_base_index(base: NonNull<T>, index: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(index + 1)\n        } else {\n            base.as_ptr().sub(index)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[inline]\n    unsafe fn to_base_index(&self, base: NonNull<T>) -> usize {}\n    #[inline]\n    pub fn as_ptr(&self) -> *mut T {}\n    #[inline]\n    fn as_non_null(&self) -> NonNull<T> {}\n    #[inline]\n    unsafe fn next_n(&self, offset: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(self.ptr.as_ptr() as usize + offset)\n        } else {\n            self.ptr.as_ptr().sub(offset)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    pub(crate) unsafe fn drop(&self) {}\n    #[inline]\n    pub(crate) unsafe fn read(&self) -> T {}\n    #[inline]\n    pub(crate) unsafe fn write(&self, val: T) {}\n    #[inline]\n    pub unsafe fn as_ref<'a>(&self) -> &'a T {}\n    #[inline]\n    pub unsafe fn as_mut<'a>(&self) -> &'a mut T {}\n}\n\nThe function to be tested is presented as follows:\n/// # Safety\n/// If `DO_CHECK_PTR_RANGE` is false, caller must ensure that we never try to iterate\n/// after yielding all elements.\nunsafe fn next_impl<const DO_CHECK_PTR_RANGE: bool>(&mut self) -> Option<Bucket<T>> {\n    loop {\n        if let Some(index) = self.current_group.next() {\n            return Some(self.data.next_n(index));\n        }\n\n        if DO_CHECK_PTR_RANGE && self.next_ctrl >= self.end {\n            return None;\n        }\n\n        // We might read past self.end up to the next group boundary,\n        // but this is fine because it only occurs on tables smaller\n        // than the group size where the trailing control bytes are all\n        // EMPTY. On larger tables self.end is guaranteed to be aligned\n        // to the group size (since tables are power-of-two sized).\n        self.current_group = Group::load_aligned(self.next_ctrl.cast())\n            .match_full()\n            .into_iter();\n        self.data = self.data.next_n(Group::WIDTH);\n        self.next_ctrl = self.next_ctrl.add(Group::WIDTH);\n    }\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}