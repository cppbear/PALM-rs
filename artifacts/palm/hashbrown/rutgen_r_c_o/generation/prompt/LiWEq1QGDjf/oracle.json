{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, do not repeat provided test prefixes, avoid additional explanations, and do not use Markdown.\n2. Generate necessary test oracles solely for the provided test prefixes.\n3. Each test oracle's assertions are independent and have no dependencies between them.\n4. Group all non-assertion statements first, followed by all assertion statements.\n5. Generate test oracles by interpreting the behavior of the test function through the provided test prefixes, context, and documentation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\npub(crate) struct FullBucketsIndices {\n    current_group: BitMaskIter,\n    group_first_index: usize,\n    ctrl: NonNull<u8>,\n    items: usize,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMaskIter(pub(crate) BitMask);\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\n#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n#[repr(transparent)]\npub(crate) struct Tag(u8);\nimpl FullBucketsIndices {\n    #[inline(always)]\n    unsafe fn next_impl(&mut self) -> Option<usize> {\n        loop {\n            if let Some(index) = self.current_group.next() {\n                return Some(self.group_first_index + index);\n            }\n            self.ctrl = NonNull::new_unchecked(self.ctrl.as_ptr().add(Group::WIDTH));\n            self.current_group = Group::load_aligned(self.ctrl.as_ptr().cast())\n                .match_full()\n                .into_iter();\n            self.group_first_index += Group::WIDTH;\n        }\n    }\n}\nimpl Iterator for BitMaskIter {\n    type Item = usize;\n    #[inline]\n    fn next(&mut self) -> Option<usize> {\n        let bit = self.0.lowest_set_bit()?;\n        self.0 = self.0.remove_lowest_bit();\n        Some(bit)\n    }\n}\nimpl IntoIterator for BitMask {\n    type Item = usize;\n    type IntoIter = BitMaskIter;\n    #[inline]\n    fn into_iter(self) -> BitMaskIter {\n        BitMaskIter(BitMask(self.0 & BITMASK_ITER_MASK))\n    }\n}\n\nThe function to be tested is presented as follows:\n/// Advances the iterator and returns the next value.\n///\n/// # Safety\n///\n/// If any of the following conditions are violated, the result is\n/// [`Undefined Behavior`]:\n///\n/// * The [`RawTableInner`] / [`RawTable`] must be alive and not moved,\n///   i.e. table outlives the `FullBucketsIndices`;\n///\n/// * It never tries to iterate after getting all elements.\n///\n/// [`Undefined Behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\nunsafe fn next_impl(&mut self) -> Option<usize> {\n    loop {\n        if let Some(index) = self.current_group.next() {\n            // The returned `self.group_first_index + index` will always\n            // be in the range `0..self.buckets()`. See explanation below.\n            return Some(self.group_first_index + index);\n        }\n\n        // SAFETY: The caller of this function ensures that:\n        //\n        // 1. It never tries to iterate after getting all the elements;\n        // 2. The table is alive and did not moved;\n        // 3. The first `self.ctrl` pointed to the start of the array of control bytes.\n        //\n        // Taking the above into account, we always stay within the bounds, because:\n        //\n        // 1. For tables smaller than the group width (self.buckets() <= Group::WIDTH),\n        //    we will never end up in the given branch, since we should have already\n        //    yielded all the elements of the table.\n        //\n        // 2. For tables larger than the group width. The number of buckets is a\n        //    power of two (2 ^ n), Group::WIDTH is also power of two (2 ^ k). Since\n        //    `(2 ^ n) > (2 ^ k)`, than `(2 ^ n) % (2 ^ k) = 0`. As we start from the\n        //    the start of the array of control bytes, and never try to iterate after\n        //    getting all the elements, the last `self.ctrl` will be equal to\n        //    the `self.buckets() - Group::WIDTH`, so `self.current_group.next()`\n        //    will always contains indices within the range `0..Group::WIDTH`,\n        //    and subsequent `self.group_first_index + index` will always return a\n        //    number less than `self.buckets()`.\n        self.ctrl = NonNull::new_unchecked(self.ctrl.as_ptr().add(Group::WIDTH));\n\n        // SAFETY: See explanation above.\n        self.current_group = Group::load_aligned(self.ctrl.as_ptr().cast())\n            .match_full()\n            .into_iter();\n        self.group_first_index += Group::WIDTH;\n    }\n}\n",
  "depend_pt": ""
}