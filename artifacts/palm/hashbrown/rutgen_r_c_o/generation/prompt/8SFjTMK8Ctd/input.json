{
  "system_pt": "As a software testing expert, infer the test input ranges based on the provided information. Follow these guidelines:\n1. Provide test input ranges in one line in plain text only, without additional explanations or Markdown formatting.\n2. The inferred test input ranges should only satisfy all provided constraints simultaneously.\n3. Ensure the test input ranges cover boundary cases and edge scenarios.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\n#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n#[repr(transparent)]\npub(crate) struct Tag(u8);\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl(0).write_bytes(Tag::EMPTY.0, result.num_ctrl_bytes());\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {}\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {}\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {}\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {\n        let index2 = ((index.wrapping_sub(Group::WIDTH)) & self.bucket_mask)\n            + Group::WIDTH;\n        *self.ctrl(index) = ctrl;\n        *self.ctrl(index2) = ctrl;\n    }\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {\n        debug_assert!(index < self.num_ctrl_bytes());\n        self.ctrl.as_ptr().add(index).cast()\n    }\n    #[inline]\n    fn buckets(&self) -> usize {}\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {\n        debug_assert!(index < self.buckets());\n        (*self.ctrl(index)).is_full()\n    }\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {}\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {\n        debug_assert!(self.is_bucket_full(index));\n        let index_before = index.wrapping_sub(Group::WIDTH) & self.bucket_mask;\n        let empty_before = Group::load(self.ctrl(index_before)).match_empty();\n        let empty_after = Group::load(self.ctrl(index)).match_empty();\n        let ctrl = if empty_before.leading_zeros() + empty_after.trailing_zeros()\n            >= Group::WIDTH\n        {\n            Tag::DELETED\n        } else {\n            self.growth_left += 1;\n            Tag::EMPTY\n        };\n        self.set_ctrl(index, ctrl);\n        self.items -= 1;\n    }\n}\n#[allow(clippy::use_self)]\nimpl BitMask {\n    #[inline]\n    #[must_use]\n    #[allow(dead_code)]\n    pub(crate) fn invert(self) -> Self {\n        BitMask(self.0 ^ BITMASK_MASK)\n    }\n    #[inline]\n    #[must_use]\n    fn remove_lowest_bit(self) -> Self {\n        BitMask(self.0 & (self.0 - 1))\n    }\n    #[inline]\n    pub(crate) fn any_bit_set(self) -> bool {}\n    #[inline]\n    pub(crate) fn lowest_set_bit(self) -> Option<usize> {}\n    #[inline]\n    pub(crate) fn trailing_zeros(self) -> usize {\n        if cfg!(target_arch = \"arm\") && BITMASK_STRIDE % 8 == 0 {\n            self.0.swap_bytes().leading_zeros() as usize / BITMASK_STRIDE\n        } else {\n            self.0.trailing_zeros() as usize / BITMASK_STRIDE\n        }\n    }\n    #[inline]\n    fn nonzero_trailing_zeros(nonzero: NonZeroBitMaskWord) -> usize {}\n    #[inline]\n    pub(crate) fn leading_zeros(self) -> usize {\n        self.0.leading_zeros() as usize / BITMASK_STRIDE\n    }\n}\n\nThe function to be tested is presented as follows:\n/// Erases the [`Bucket`]'s control byte at the given index so that it does not\n/// triggered as full, decreases the `items` of the table and, if it can be done,\n/// increases `self.growth_left`.\n///\n/// This function does not actually erase / drop the [`Bucket`] itself, i.e. it\n/// does not make any changes to the `data` parts of the table. The caller of this\n/// function must take care to properly drop the `data`, otherwise calling this\n/// function may result in a memory leak.\n///\n/// # Safety\n///\n/// You must observe the following safety rules when calling this function:\n///\n/// * The [`RawTableInner`] has already been allocated;\n///\n/// * It must be the full control byte at the given position;\n///\n/// * The `index` must not be greater than the `RawTableInner.bucket_mask`, i.e.\n///   `index <= RawTableInner.bucket_mask` or, in other words, `(index + 1)` must\n///   be no greater than the number returned by the function [`RawTableInner::buckets`].\n///\n/// Calling this function on a table that has not been allocated results in [`undefined behavior`].\n///\n/// Calling this function on a table with no elements is unspecified, but calling subsequent\n/// functions is likely to result in [`undefined behavior`] due to overflow subtraction\n/// (`self.items -= 1 cause overflow when self.items == 0`).\n///\n/// See also [`Bucket::as_ptr`] method, for more information about of properly removing\n/// or saving `data element` from / into the [`RawTable`] / [`RawTableInner`].\n///\n/// [`RawTableInner::buckets`]: RawTableInner::buckets\n/// [`Bucket::as_ptr`]: Bucket::as_ptr\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\nunsafe fn erase(&mut self, index: usize) {\n    debug_assert!(self.is_bucket_full(index));\n\n    // This is the same as `index.wrapping_sub(Group::WIDTH) % self.buckets()` because\n    // the number of buckets is a power of two, and `self.bucket_mask = self.buckets() - 1`.\n    let index_before = index.wrapping_sub(Group::WIDTH) & self.bucket_mask;\n    // SAFETY:\n    // - The caller must uphold the safety contract for `erase` method;\n    // - `index_before` is guaranteed to be in range due to masking with `self.bucket_mask`\n    let empty_before = Group::load(self.ctrl(index_before)).match_empty();\n    let empty_after = Group::load(self.ctrl(index)).match_empty();\n\n    // Inserting and searching in the map is performed by two key functions:\n    //\n    // - The `find_insert_slot` function that looks up the index of any `Tag::EMPTY` or `Tag::DELETED`\n    //   slot in a group to be able to insert. If it doesn't find an `Tag::EMPTY` or `Tag::DELETED`\n    //   slot immediately in the first group, it jumps to the next `Group` looking for it,\n    //   and so on until it has gone through all the groups in the control bytes.\n    //\n    // - The `find_inner` function that looks for the index of the desired element by looking\n    //   at all the `FULL` bytes in the group. If it did not find the element right away, and\n    //   there is no `Tag::EMPTY` byte in the group, then this means that the `find_insert_slot`\n    //   function may have found a suitable slot in the next group. Therefore, `find_inner`\n    //   jumps further, and if it does not find the desired element and again there is no `Tag::EMPTY`\n    //   byte, then it jumps further, and so on. The search stops only if `find_inner` function\n    //   finds the desired element or hits an `Tag::EMPTY` slot/byte.\n    //\n    // Accordingly, this leads to two consequences:\n    //\n    // - The map must have `Tag::EMPTY` slots (bytes);\n    //\n    // - You can't just mark the byte to be erased as `Tag::EMPTY`, because otherwise the `find_inner`\n    //   function may stumble upon an `Tag::EMPTY` byte before finding the desired element and stop\n    //   searching.\n    //\n    // Thus it is necessary to check all bytes after and before the erased element. If we are in\n    // a contiguous `Group` of `FULL` or `Tag::DELETED` bytes (the number of `FULL` or `Tag::DELETED` bytes\n    // before and after is greater than or equal to `Group::WIDTH`), then we must mark our byte as\n    // `Tag::DELETED` in order for the `find_inner` function to go further. On the other hand, if there\n    // is at least one `Tag::EMPTY` slot in the `Group`, then the `find_inner` function will still stumble\n    // upon an `Tag::EMPTY` byte, so we can safely mark our erased byte as `Tag::EMPTY` as well.\n    //\n    // Finally, since `index_before == (index.wrapping_sub(Group::WIDTH) & self.bucket_mask) == index`\n    // and given all of the above, tables smaller than the group width (self.buckets() < Group::WIDTH)\n    // cannot have `Tag::DELETED` bytes.\n    //\n    // Note that in this context `leading_zeros` refers to the bytes at the end of a group, while\n    // `trailing_zeros` refers to the bytes at the beginning of a group.\n    let ctrl = if empty_before.leading_zeros() + empty_after.trailing_zeros() >= Group::WIDTH {\n        Tag::DELETED\n    } else {\n        self.growth_left += 1;\n        Tag::EMPTY\n    };\n    // SAFETY: the caller must uphold the safety contract for `erase` method.\n    self.set_ctrl(index, ctrl);\n    self.items -= 1;\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}