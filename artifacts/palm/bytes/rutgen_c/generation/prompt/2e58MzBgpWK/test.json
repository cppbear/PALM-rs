{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop};\nuse core::ops::{Deref, RangeBounds};\nuse core::ptr::NonNull;\nuse core::{cmp, fmt, hash, ptr, slice, usize};\nuse alloc::{\n    alloc::{dealloc, Layout},\n    borrow::Borrow, boxed::Box, string::String, vec::Vec,\n};\nuse crate::buf::IntoIter;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BytesMut};\nstatic OWNED_VTABLE: Vtable = Vtable {\n    clone: owned_clone,\n    into_vec: owned_to_vec,\n    into_mut: owned_to_mut,\n    is_unique: owned_is_unique,\n    drop: owned_drop,\n};\nstatic PROMOTABLE_EVEN_VTABLE: Vtable = Vtable {\n    clone: promotable_even_clone,\n    into_vec: promotable_even_to_vec,\n    into_mut: promotable_even_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_even_drop,\n};\nstatic PROMOTABLE_ODD_VTABLE: Vtable = Vtable {\n    clone: promotable_odd_clone,\n    into_vec: promotable_odd_to_vec,\n    into_mut: promotable_odd_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_odd_drop,\n};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_clone,\n    into_vec: shared_to_vec,\n    into_mut: shared_to_mut,\n    is_unique: shared_is_unique,\n    drop: shared_drop,\n};\nconst STATIC_VTABLE: Vtable = Vtable {\n    clone: static_clone,\n    into_vec: static_to_vec,\n    into_mut: static_to_mut,\n    is_unique: static_is_unique,\n    drop: static_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\npub struct Bytes {\n    ptr: *const u8,\n    len: usize,\n    data: AtomicPtr<()>,\n    vtable: &'static Vtable,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nunsafe fn promotable_even_clone(\n    data: &AtomicPtr<()>,\n    ptr: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = data.load(Ordering::Acquire);\n    let kind = shared as usize & KIND_MASK;\n    if kind == KIND_ARC {\n        shallow_clone_arc(shared.cast(), ptr, len)\n    } else {\n        debug_assert_eq!(kind, KIND_VEC);\n        let buf = ptr_map(shared.cast(), |addr| addr & !KIND_MASK);\n        shallow_clone_vec(data, shared, buf, ptr, len)\n    }\n}\n#[cfg(not(miri))]\nfn ptr_map<F>(ptr: *mut u8, f: F) -> *mut u8\nwhere\n    F: FnOnce(usize) -> usize,\n{\n    let old_addr = ptr as usize;\n    let new_addr = f(old_addr);\n    new_addr as *mut u8\n}\nunsafe fn shallow_clone_arc(shared: *mut Shared, ptr: *const u8, len: usize) -> Bytes {\n    let old_size = (*shared).ref_cnt.fetch_add(1, Ordering::Relaxed);\n    if old_size > usize::MAX >> 1 {\n        crate::abort();\n    }\n    Bytes {\n        ptr,\n        len,\n        data: AtomicPtr::new(shared as _),\n        vtable: &SHARED_VTABLE,\n    }\n}\n#[cold]\nunsafe fn shallow_clone_vec(\n    atom: &AtomicPtr<()>,\n    ptr: *const (),\n    buf: *mut u8,\n    offset: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = Box::new(Shared {\n        buf,\n        cap: offset_from(offset, buf) + len,\n        ref_cnt: AtomicUsize::new(2),\n    });\n    let shared = Box::into_raw(shared);\n    debug_assert!(\n        0 == (shared as usize & KIND_MASK),\n        \"internal: Box<Shared> should have an aligned pointer\",\n    );\n    match atom\n        .compare_exchange(ptr as _, shared as _, Ordering::AcqRel, Ordering::Acquire)\n    {\n        Ok(actual) => {\n            debug_assert!(actual as usize == ptr as usize);\n            Bytes {\n                ptr: offset,\n                len,\n                data: AtomicPtr::new(shared as _),\n                vtable: &SHARED_VTABLE,\n            }\n        }\n        Err(actual) => {\n            let shared = Box::from_raw(shared);\n            mem::forget(*shared);\n            shallow_clone_arc(actual as _, offset, len)\n        }\n    }\n}\n\nThe function to be tested is presented as follows:\nunsafe fn promotable_even_clone(data: &AtomicPtr<()>, ptr: *const u8, len: usize) -> Bytes {\n    let shared = data.load(Ordering::Acquire);\n    let kind = shared as usize & KIND_MASK;\n\n    if kind == KIND_ARC {\n        shallow_clone_arc(shared.cast(), ptr, len)\n    } else {\n        debug_assert_eq!(kind, KIND_VEC);\n        let buf = ptr_map(shared.cast(), |addr| addr & !KIND_MASK);\n        shallow_clone_vec(data, shared, buf, ptr, len)\n    }\n}\n",
  "depend_pt": ""
}