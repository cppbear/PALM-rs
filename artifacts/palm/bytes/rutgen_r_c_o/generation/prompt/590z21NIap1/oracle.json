{
  "system_pt": "As a software testing expert, please generate accurate test oracles code based on the provided information. Follow these guidelines:\n1. Generate executable test oracle codes in plain text format, one per line. Do not output complete test functions, do not repeat provided test prefixes, avoid additional explanations, and do not use Markdown.\n2. Generate necessary test oracles solely for the provided test prefixes.\n3. Each test oracle's assertions are independent and have no dependencies between them.\n4. Group all non-assertion statements first, followed by all assertion statements.\n5. Generate test oracles by interpreting the behavior of the test function through the provided test prefixes, context, and documentation.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes_mut.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop, MaybeUninit};\nuse core::ops::{Deref, DerefMut};\nuse core::ptr::{self, NonNull};\nuse core::{cmp, fmt, hash, isize, slice, usize};\nuse alloc::{\n    borrow::{Borrow, BorrowMut},\n    boxed::Box, string::String, vec, vec::Vec,\n};\nuse crate::buf::{IntoIter, UninitSlice};\nuse crate::bytes::Vtable;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BufMut, Bytes, TryGetError};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_v_clone,\n    into_vec: shared_v_to_vec,\n    into_mut: shared_v_to_mut,\n    is_unique: shared_v_is_unique,\n    drop: shared_v_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nconst MAX_ORIGINAL_CAPACITY_WIDTH: usize = 17;\nconst MIN_ORIGINAL_CAPACITY_WIDTH: usize = 10;\nconst ORIGINAL_CAPACITY_MASK: usize = 0b11100;\nconst ORIGINAL_CAPACITY_OFFSET: usize = 2;\nconst VEC_POS_OFFSET: usize = 5;\nconst MAX_VEC_POS: usize = usize::MAX >> VEC_POS_OFFSET;\nconst NOT_VEC_POS_MASK: usize = 0b11111;\n#[cfg(target_pointer_width = \"64\")]\nconst PTR_WIDTH: usize = 64;\n#[cfg(target_pointer_width = \"32\")]\nconst PTR_WIDTH: usize = 32;\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nstruct Shared {\n    vec: Vec<u8>,\n    original_capacity_repr: usize,\n    ref_count: AtomicUsize,\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {}\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {}\n    #[inline]\n    pub fn freeze(self) -> Bytes {}\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {}\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {}\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {}\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {}\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {\n        if count == 0 {\n            return;\n        }\n        debug_assert!(count <= self.cap, \"internal: set_start out of bounds\");\n        let kind = self.kind();\n        if kind == KIND_VEC {\n            let pos = self.get_vec_pos() + count;\n            if pos <= MAX_VEC_POS {\n                self.set_vec_pos(pos);\n            } else {\n                self.promote_to_shared(1);\n            }\n        }\n        self.ptr = vptr(self.ptr.as_ptr().add(count));\n        self.len = self.len.checked_sub(count).unwrap_or(0);\n        self.cap -= count;\n    }\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {\n        self.data as usize & KIND_MASK\n    }\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {\n        debug_assert_eq!(self.kind(), KIND_VEC);\n        debug_assert!(ref_cnt == 1 || ref_cnt == 2);\n        let original_capacity_repr = (self.data as usize & ORIGINAL_CAPACITY_MASK)\n            >> ORIGINAL_CAPACITY_OFFSET;\n        let off = (self.data as usize) >> VEC_POS_OFFSET;\n        let shared = Box::new(Shared {\n            vec: rebuild_vec(self.ptr.as_ptr(), self.len, self.cap, off),\n            original_capacity_repr,\n            ref_count: AtomicUsize::new(ref_cnt),\n        });\n        let shared = Box::into_raw(shared);\n        debug_assert_eq!(shared as usize & KIND_MASK, KIND_ARC);\n        self.data = shared;\n    }\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {\n        debug_assert_eq!(self.kind(), KIND_VEC);\n        self.data as usize >> VEC_POS_OFFSET\n    }\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {\n        debug_assert_eq!(self.kind(), KIND_VEC);\n        debug_assert!(pos <= MAX_VEC_POS);\n        self.data = invalid_ptr(\n            (pos << VEC_POS_OFFSET) | (self.data as usize & NOT_VEC_POS_MASK),\n        );\n    }\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\n#[inline]\nfn vptr(ptr: *mut u8) -> NonNull<u8> {\n    if cfg!(debug_assertions) {\n        NonNull::new(ptr).expect(\"Vec pointer should be non-null\")\n    } else {\n        unsafe { NonNull::new_unchecked(ptr) }\n    }\n}\n\nThe function to be tested is presented as follows:\n/// Advance the buffer without bounds checking.\n///\n/// # SAFETY\n///\n/// The caller must ensure that `count` <= `self.cap`.\npub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {\n    // Setting the start to 0 is a no-op, so return early if this is the\n    // case.\n    if count == 0 {\n        return;\n    }\n\n    debug_assert!(count <= self.cap, \"internal: set_start out of bounds\");\n\n    let kind = self.kind();\n\n    if kind == KIND_VEC {\n        // Setting the start when in vec representation is a little more\n        // complicated. First, we have to track how far ahead the\n        // \"start\" of the byte buffer from the beginning of the vec. We\n        // also have to ensure that we don't exceed the maximum shift.\n        let pos = self.get_vec_pos() + count;\n\n        if pos <= MAX_VEC_POS {\n            self.set_vec_pos(pos);\n        } else {\n            // The repr must be upgraded to ARC. This will never happen\n            // on 64 bit systems and will only happen on 32 bit systems\n            // when shifting past 134,217,727 bytes. As such, we don't\n            // worry too much about performance here.\n            self.promote_to_shared(/*ref_count = */ 1);\n        }\n    }\n\n    // Updating the start of the view is setting `ptr` to point to the\n    // new start and updating the `len` field to reflect the new length\n    // of the view.\n    self.ptr = vptr(self.ptr.as_ptr().add(count));\n    self.len = self.len.checked_sub(count).unwrap_or(0);\n    self.cap -= count;\n}\n",
  "depend_pt": ""
}