{"function_name":"quote::to_tokens::<proc_macro2::TokenTree as to_tokens::ToTokens>::to_tokens","file_path":"/home/abezbm/rust-utgen-test-crates-new/quote/src/to_tokens.rs","work_dir":"/home/abezbm/rust-utgen-test-crates-new/quote","tests":28,"tests_lines":[8,9,9,9,8,9,9,9,8,8,9,9,9,9,11,11,11,8,9,8,8,8,9,8,9,9,9,9],"oracles":9,"oracles_compiled":9,"oracles_compiled_rate":100.0,"tests_compiled":27,"tests_compiled_rate":96.42857142857143,"oracles_run":9,"oracles_passed":6,"oracles_passed_rate":66.66666666666666,"tests_run":27,"tests_passed":19,"tests_passed_rate":70.37037037037037,"lines":3,"lines_covered":3,"lines_coveraged_rate":100.0,"branches":1,"branches_covered":1,"branches_coverage_rate":100.0,"codes_lines":[258,259,260],"codes_lines_covered":[[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   assert_eq!(tokens.is_empty(), true);  ","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   assert_eq!(tokens.is_empty(), false);  ","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   assert!(tokens.to_string().contains(\"valid_literal\"));  ","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   token.to_tokens(&mut tokens);  ","   assert_eq!(tokens.clone().into_iter().count(), 1);  ","}"],[258,259,260]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    assert_eq!(tokens.to_string(), \"\");","}"],[258,259,260]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    assert!(!tokens.is_empty());","}"],[258,259,260]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    assert_eq!(tokens.to_string(), \"valid_ident\");","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string().len(), 1);","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   assert_eq!(tokens.is_empty(), true);","}"],[258,259,260]],[["{","   use proc_macro2::Spacing;","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.is_empty(), false);","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string(), \";\");","}"],[258,259,260]],[["{","   use proc_macro2::Spacing; // Added import for Spacing enum","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","   token.to_tokens(&mut tokens);","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();  ","   use proc_macro2::Spacing;  ","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));  ","   assert!(tokens.into_iter().next().is_some());  ","}"],[258,259,260]],[["{","  use proc_macro2::Delimiter;  ","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::string(\"valid_group\"))));  ","  let token = TokenTree::from(group);  ","  token.to_tokens(&mut tokens);  ","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::string(\"valid_group\"))));  ","   token.to_tokens(&mut tokens);  ","   assert!(!tokens.is_empty());  ","}"],[258,259,260]],[["{","  use proc_macro2::Delimiter;  // Add this import statement","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::u8_suffixed(1))));  ","  let token = TokenTree::from(group);  ","  token.to_tokens(&mut tokens);  ","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::u8_suffixed(1))));  ","  let token = TokenTree::from(group);  ","  token.to_tokens(&mut tokens);  ","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(0));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(0));  ","   assert!(tokens.is_empty());  ","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(0)); // Use a valid suffixed literal","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(0)); // Use a valid suffixed literal","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string(), \"\");","}"],[258,259,260]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    assert!(!tokens.is_empty());","}"],[258,259,260]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    assert_eq!(tokens.to_string(), \"span\");","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","   token.to_tokens(&mut tokens);","   let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string().len(), 1);","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"boundary_case\"));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"boundary_case\"));  ","   let expected = TokenStream::from(TokenTree::from(Literal::string(\"boundary_case\")));  ","   assert_eq!(tokens.to_string(), expected.to_string());  ","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"extreme_case\"));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"extreme_case\"));","   assert_eq!(tokens.is_empty(), true);","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(255));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(255));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.is_empty(), false);","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u32_suffixed(0)); // Placeholder for the extreme_case literal","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u32_suffixed(0)); // Placeholder for the extreme_case literal","   token.to_tokens(&mut tokens);","   assert!(tokens.to_string().contains(\"0\"));","}"],[258,259,260]],[["{","  let mut tokens = TokenStream::new();  ","  let token = TokenTree::from(Literal::string(\"extreme_case\"));  ","  token.to_tokens(&mut tokens);  ","  let mut tokens = TokenStream::new();  ","  let token = TokenTree::from(Literal::string(\"extreme_case\"));  ","   token.to_tokens(&mut tokens);  ","   assert!(!tokens.is_empty());  ","}"],[258,259,260]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"token_with_special_characters$\"));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"token_with_special_characters$\"));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string(), \"token_with_special_characters$\");","}"],[258,259,260]]],"codes_branches":[],"codes_branches_covered":[[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   assert_eq!(tokens.is_empty(), true);  ","}"],[]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   assert_eq!(tokens.is_empty(), false);  ","}"],[]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"valid_literal\"));  ","   token.to_tokens(&mut tokens);  ","   assert!(tokens.to_string().contains(\"valid_literal\"));  ","}"],[]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(1));  ","   token.to_tokens(&mut tokens);  ","   assert_eq!(tokens.clone().into_iter().count(), 1);  ","}"],[]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    assert_eq!(tokens.to_string(), \"\");","}"],[]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    assert!(!tokens.is_empty());","}"],[]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","    token.to_tokens(&mut tokens);","    assert_eq!(tokens.to_string(), \"valid_ident\");","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Ident::new(\"valid_ident\", Span::call_site()));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string().len(), 1);","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   assert_eq!(tokens.is_empty(), true);","}"],[]],[["{","   use proc_macro2::Spacing;","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.is_empty(), false);","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', proc_macro2::Spacing::Alone));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string(), \";\");","}"],[]],[["{","   use proc_macro2::Spacing; // Added import for Spacing enum","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));","   token.to_tokens(&mut tokens);","}"],[]],[["{","   let mut tokens = TokenStream::new();  ","   use proc_macro2::Spacing;  ","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Punct::new(';', Spacing::Alone));  ","   assert!(tokens.into_iter().next().is_some());  ","}"],[]],[["{","  use proc_macro2::Delimiter;  ","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::string(\"valid_group\"))));  ","  let token = TokenTree::from(group);  ","  token.to_tokens(&mut tokens);  ","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::string(\"valid_group\"))));  ","   token.to_tokens(&mut tokens);  ","   assert!(!tokens.is_empty());  ","}"],[]],[["{","  use proc_macro2::Delimiter;  // Add this import statement","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::u8_suffixed(1))));  ","  let token = TokenTree::from(group);  ","  token.to_tokens(&mut tokens);  ","  let mut tokens = TokenStream::new();  ","  let group = Group::new(Delimiter::Parenthesis, TokenStream::from(TokenTree::from(Literal::u8_suffixed(1))));  ","  let token = TokenTree::from(group);  ","  token.to_tokens(&mut tokens);  ","}"],[]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(0));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::u8_suffixed(0));  ","   assert!(tokens.is_empty());  ","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(0)); // Use a valid suffixed literal","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(0)); // Use a valid suffixed literal","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string(), \"\");","}"],[]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    assert!(!tokens.is_empty());","}"],[]],[["{","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    token.to_tokens(&mut tokens);","    let mut tokens = TokenStream::new();","    let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","    assert_eq!(tokens.to_string(), \"span\");","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","   token.to_tokens(&mut tokens);","   let token = TokenTree::from(Ident::new(\"span\", Span::call_site()));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string().len(), 1);","}"],[]],[["{","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"boundary_case\"));  ","   token.to_tokens(&mut tokens);  ","   let mut tokens = TokenStream::new();  ","   let token = TokenTree::from(Literal::string(\"boundary_case\"));  ","   let expected = TokenStream::from(TokenTree::from(Literal::string(\"boundary_case\")));  ","   assert_eq!(tokens.to_string(), expected.to_string());  ","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"extreme_case\"));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"extreme_case\"));","   assert_eq!(tokens.is_empty(), true);","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(255));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u8_suffixed(255));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.is_empty(), false);","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u32_suffixed(0)); // Placeholder for the extreme_case literal","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::u32_suffixed(0)); // Placeholder for the extreme_case literal","   token.to_tokens(&mut tokens);","   assert!(tokens.to_string().contains(\"0\"));","}"],[]],[["{","  let mut tokens = TokenStream::new();  ","  let token = TokenTree::from(Literal::string(\"extreme_case\"));  ","  token.to_tokens(&mut tokens);  ","  let mut tokens = TokenStream::new();  ","  let token = TokenTree::from(Literal::string(\"extreme_case\"));  ","   token.to_tokens(&mut tokens);  ","   assert!(!tokens.is_empty());  ","}"],[]],[["{","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"token_with_special_characters$\"));","   token.to_tokens(&mut tokens);","   let mut tokens = TokenStream::new();","   let token = TokenTree::from(Literal::string(\"token_with_special_characters$\"));","   token.to_tokens(&mut tokens);","   assert_eq!(tokens.to_string(), \"token_with_special_characters$\");","}"],[]]]}