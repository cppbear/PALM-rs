{"function_name":"quote::ext::<proc_macro2::TokenStream as ext::TokenStreamExt>::append","file_path":"/home/abezbm/rust-utgen-test-crates-new/quote/src/ext.rs","work_dir":"/home/abezbm/rust-utgen-test-crates-new/quote","tests":10,"tests_lines":[15,19,20,15,18,16,20,21,16,21],"oracles":4,"oracles_compiled":3,"oracles_compiled_rate":75.0,"tests_compiled":5,"tests_compiled_rate":50.0,"oracles_run":3,"oracles_passed":2,"oracles_passed_rate":66.66666666666666,"tests_run":5,"tests_passed":2,"tests_passed_rate":40.0,"lines":6,"lines_covered":6,"lines_coveraged_rate":100.0,"branches":1,"branches_covered":1,"branches_coverage_rate":100.0,"codes_lines":[60,61,62,63,64,65],"codes_lines_covered":[[["{","  #[should_panic]","  fn test_append_invalid_type() {","      let mut tokens = TokenStream::new();","      let invalid_token: TokenTree = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));","      tokens.append(invalid_token);","  }","      let mut tokens = TokenStream::new();","      let token: TokenTree = TokenTree::Punct(proc_macro2::Punct::new(',', proc_macro2::Spacing::Alone));","      tokens.append(token);","      let mut tokens = TokenStream::new();","      let invalid_token: TokenTree = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));","      let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| { tokens.append(invalid_token); }));","      assert!(result.is_err());","}"],[60,61,62,63,64,65]],[["{","  #[should_panic]  ","  fn test_append_invalid_type() {  ","      let mut tokens = TokenStream::new();  ","      let invalid_token = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));  ","      tokens.append(invalid_token);  ","  }  ","      let mut tokens = TokenStream::new();  ","      let token: TokenTree = TokenTree::Punct(proc_macro2::Punct::new(',', proc_macro2::Spacing::Alone));  ","      tokens.append(token);  ","      let mut tokens = TokenStream::new();  ","      let invalid_token = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));  ","      let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| { tokens.append(invalid_token); }));  ","      let mut tokens = TokenStream::new();  ","      let token: TokenTree = TokenTree::Punct(proc_macro2::Punct::new(',', proc_macro2::Spacing::Alone));  ","      tokens.append(token);  ","      assert_eq!(tokens.to_string(), \",\");  ","}"],[60,61,62,63,64,65]],[["{","#[should_panic]","fn test_append_invalid_type() {","   let mut tokens = TokenStream::new();","   // The following line has been removed because invalid_token is not a valid token type.","   // tokens.append(invalid_token);","}","   let mut tokens = TokenStream::new();","   let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","  let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","  tokens.append(token1);","  tokens.append(token2);","  // The invalid_token definition is not needed anymore, so removed it.","  // let invalid_token: i32 = 42;","  let mut tokens = TokenStream::new();","  let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","  let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","  tokens.append(token1);","","}"],[60,61,62,63,64,65]],[["{","   #[should_panic]","   fn test_append_invalid_type() {","       let mut tokens = TokenStream::new();","       let invalid_token: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(42)); ","       tokens.append(invalid_token);","}","    let mut tokens = TokenStream::new();","    let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","   let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","   tokens.append(token1);","   tokens.append(token2);","   let mut tokens = TokenStream::new();","   let invalid_token: i32 = 42;","   let mut tokens = TokenStream::new();","   let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","   let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","   tokens.append(token1);","   tokens.append(token2);","   assert_eq!(tokens.to_string(), \"first_token123\");","}"],[60,61,62,63,64,65]],[["{","#[should_panic]","fn test_append_invalid_type() {","   let mut tokens = TokenStream::new();","   let invalid_token: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"invalid_token\", proc_macro2::Span::call_site()));","   tokens.append(invalid_token);","}","    let mut tokens = TokenStream::new();","    for i in 0..1000 {","        let token: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(&format!(\"token_{}\", i), proc_macro2::Span::call_site()));","        tokens.append(token);","    }","    let mut tokens = TokenStream::new();","    let invalid_token: i32 = 42;","    let mut tokens = TokenStream::new();","    for i in 0..1000 {","    let token: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(&format!(\"token_{}\", i), proc_macro2::Span::call_site()));","    tokens.append(token);","    }","    assert_eq!(tokens.to_string(), (0..1000).map(|i| format!(\"token_{}\", i)).collect::<Vec<_>>().join(\"\"));","}"],[60,61,62,63,64,65]]],"codes_branches":[],"codes_branches_covered":[[["{","  #[should_panic]","  fn test_append_invalid_type() {","      let mut tokens = TokenStream::new();","      let invalid_token: TokenTree = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));","      tokens.append(invalid_token);","  }","      let mut tokens = TokenStream::new();","      let token: TokenTree = TokenTree::Punct(proc_macro2::Punct::new(',', proc_macro2::Spacing::Alone));","      tokens.append(token);","      let mut tokens = TokenStream::new();","      let invalid_token: TokenTree = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));","      let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| { tokens.append(invalid_token); }));","      assert!(result.is_err());","}"],[]],[["{","  #[should_panic]  ","  fn test_append_invalid_type() {  ","      let mut tokens = TokenStream::new();  ","      let invalid_token = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));  ","      tokens.append(invalid_token);  ","  }  ","      let mut tokens = TokenStream::new();  ","      let token: TokenTree = TokenTree::Punct(proc_macro2::Punct::new(',', proc_macro2::Spacing::Alone));  ","      tokens.append(token);  ","      let mut tokens = TokenStream::new();  ","      let invalid_token = TokenTree::Literal(proc_macro2::Literal::i32_suffixed(42));  ","      let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| { tokens.append(invalid_token); }));  ","      let mut tokens = TokenStream::new();  ","      let token: TokenTree = TokenTree::Punct(proc_macro2::Punct::new(',', proc_macro2::Spacing::Alone));  ","      tokens.append(token);  ","      assert_eq!(tokens.to_string(), \",\");  ","}"],[]],[["{","#[should_panic]","fn test_append_invalid_type() {","   let mut tokens = TokenStream::new();","   // The following line has been removed because invalid_token is not a valid token type.","   // tokens.append(invalid_token);","}","   let mut tokens = TokenStream::new();","   let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","  let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","  tokens.append(token1);","  tokens.append(token2);","  // The invalid_token definition is not needed anymore, so removed it.","  // let invalid_token: i32 = 42;","  let mut tokens = TokenStream::new();","  let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","  let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","  tokens.append(token1);","","}"],[]],[["{","   #[should_panic]","   fn test_append_invalid_type() {","       let mut tokens = TokenStream::new();","       let invalid_token: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(42)); ","       tokens.append(invalid_token);","}","    let mut tokens = TokenStream::new();","    let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","   let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","   tokens.append(token1);","   tokens.append(token2);","   let mut tokens = TokenStream::new();","   let invalid_token: i32 = 42;","   let mut tokens = TokenStream::new();","   let token1: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"first_token\", proc_macro2::Span::call_site()));","   let token2: TokenTree = TokenTree::Literal(proc_macro2::Literal::u32_unsuffixed(123));","   tokens.append(token1);","   tokens.append(token2);","   assert_eq!(tokens.to_string(), \"first_token123\");","}"],[]],[["{","#[should_panic]","fn test_append_invalid_type() {","   let mut tokens = TokenStream::new();","   let invalid_token: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(\"invalid_token\", proc_macro2::Span::call_site()));","   tokens.append(invalid_token);","}","    let mut tokens = TokenStream::new();","    for i in 0..1000 {","        let token: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(&format!(\"token_{}\", i), proc_macro2::Span::call_site()));","        tokens.append(token);","    }","    let mut tokens = TokenStream::new();","    let invalid_token: i32 = 42;","    let mut tokens = TokenStream::new();","    for i in 0..1000 {","    let token: TokenTree = TokenTree::Ident(proc_macro2::Ident::new(&format!(\"token_{}\", i), proc_macro2::Span::call_site()));","    tokens.append(token);","    }","    assert_eq!(tokens.to_string(), (0..1000).map(|i| format!(\"token_{}\", i)).collect::<Vec<_>>().join(\"\"));","}"],[]]]}