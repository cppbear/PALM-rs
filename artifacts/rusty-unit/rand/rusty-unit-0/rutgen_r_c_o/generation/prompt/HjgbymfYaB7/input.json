{
  "system_pt": "As a software testing expert, infer the test input ranges based on the provided information. Follow these guidelines:\n1. Provide test input ranges in one line in plain text only, without additional explanations or Markdown formatting.\n2. The inferred test input ranges should only satisfy all provided constraints simultaneously.\n3. Ensure the test input ranges cover boundary cases and edge scenarios.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/distr/weighted/weighted_index.rs\n// crate name is rand\nuse super::{Error, Weight};\nuse crate::distr::uniform::{SampleBorrow, SampleUniform, UniformSampler};\nuse crate::distr::Distribution;\nuse crate::Rng;\nuse alloc::vec::Vec;\nuse core::fmt::{self, Debug};\n#[cfg(feature = \"serde\")]\nuse serde::{Deserialize, Serialize};\npub trait Distribution<T> {\n    fn sample<R: Rng + ?Sized>(&self, rng: &mut R) -> T;\n    fn sample_iter<R>(self, rng: R) -> Iter<Self, R, T>\n    where\n        R: Rng,\n        Self: Sized,\n    {\n        Iter {\n            distr: self,\n            rng,\n            phantom: core::marker::PhantomData,\n        }\n    }\n    fn map<F, S>(self, func: F) -> Map<Self, F, T, S>\n    where\n        F: Fn(T) -> S,\n        Self: Sized,\n    {\n        Map {\n            distr: self,\n            func,\n            phantom: core::marker::PhantomData,\n        }\n    }\n}\npub trait SampleUniform: Sized {\n    type Sampler: UniformSampler<X = Self>;\n}\n#[derive(Debug, Clone, PartialEq)]\n#[cfg_attr(feature = \"serde\", derive(Serialize, Deserialize))]\npub struct WeightedIndex<X: SampleUniform + PartialOrd> {\n    cumulative_weights: Vec<X>,\n    total_weight: X,\n    weight_distribution: X::Sampler,\n}\n#[derive(Clone, Copy, Debug, PartialEq, Eq)]\npub enum Error {\n    /// `low > high`, or equal in case of exclusive range.\n    EmptyRange,\n    /// Input or range `high - low` is non-finite. Not relevant to integer types.\n    NonFinite,\n}\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\n#[non_exhaustive]\npub enum Error {\n    /// The input weight sequence is empty, too long, or wrongly ordered\n    InvalidInput,\n    /// A weight is negative, too large for the distribution, or not a valid number\n    InvalidWeight,\n    /// Not enough non-zero weights are available to sample values\n    ///\n    /// When attempting to sample a single value this implies that all weights\n    /// are zero. When attempting to sample `amount` values this implies that\n    /// less than `amount` weights are greater than zero.\n    InsufficientNonZero,\n    /// Overflow when calculating the sum of weights\n    Overflow,\n}\nimpl<X: SampleUniform + PartialOrd> WeightedIndex<X> {\n    pub fn new<I>(weights: I) -> Result<WeightedIndex<X>, Error>\n    where\n        I: IntoIterator,\n        I::Item: SampleBorrow<X>,\n        X: Weight,\n    {}\n    pub fn update_weights(&mut self, new_weights: &[(usize, &X)]) -> Result<(), Error>\n    where\n        X: for<'a> core::ops::AddAssign<&'a X> + for<'a> core::ops::SubAssign<&'a X>\n            + Clone + Default,\n    {\n        if new_weights.is_empty() {\n            return Ok(());\n        }\n        let zero = <X as Default>::default();\n        let mut total_weight = self.total_weight.clone();\n        let mut prev_i = None;\n        for &(i, w) in new_weights {\n            if let Some(old_i) = prev_i {\n                if old_i >= i {\n                    return Err(Error::InvalidInput);\n                }\n            }\n            if !(*w >= zero) {\n                return Err(Error::InvalidWeight);\n            }\n            if i > self.cumulative_weights.len() {\n                return Err(Error::InvalidInput);\n            }\n            let mut old_w = if i < self.cumulative_weights.len() {\n                self.cumulative_weights[i].clone()\n            } else {\n                self.total_weight.clone()\n            };\n            if i > 0 {\n                old_w -= &self.cumulative_weights[i - 1];\n            }\n            total_weight -= &old_w;\n            total_weight += w;\n            prev_i = Some(i);\n        }\n        if total_weight <= zero {\n            return Err(Error::InsufficientNonZero);\n        }\n        let mut iter = new_weights.iter();\n        let mut prev_weight = zero.clone();\n        let mut next_new_weight = iter.next();\n        let &(first_new_index, _) = next_new_weight.unwrap();\n        let mut cumulative_weight = if first_new_index > 0 {\n            self.cumulative_weights[first_new_index - 1].clone()\n        } else {\n            zero.clone()\n        };\n        for i in first_new_index..self.cumulative_weights.len() {\n            match next_new_weight {\n                Some(&(j, w)) if i == j => {\n                    cumulative_weight += w;\n                    next_new_weight = iter.next();\n                }\n                _ => {\n                    let mut tmp = self.cumulative_weights[i].clone();\n                    tmp -= &prev_weight;\n                    cumulative_weight += &tmp;\n                }\n            }\n            prev_weight = cumulative_weight.clone();\n            core::mem::swap(&mut prev_weight, &mut self.cumulative_weights[i]);\n        }\n        self.total_weight = total_weight;\n        self.weight_distribution = X::Sampler::new(zero, self.total_weight.clone())\n            .unwrap();\n        Ok(())\n    }\n}\n\nThe function to be tested is presented as follows:\n/// Update a subset of weights, without changing the number of weights.\n///\n/// `new_weights` must be sorted by the index.\n///\n/// Using this method instead of `new` might be more efficient if only a small number of\n/// weights is modified. No allocations are performed, unless the weight type `X` uses\n/// allocation internally.\n///\n/// In case of error, `self` is not modified. Error cases:\n/// -   [`Error::InvalidInput`] when `new_weights` are not ordered by\n///     index or an index is too large.\n/// -   [`Error::InvalidWeight`] when a weight is not-a-number or negative.\n/// -   [`Error::InsufficientNonZero`] when the sum of all weights is zero.\n///     Note that due to floating-point loss of precision, this case is not\n///     always correctly detected; usage of a fixed-point weight type may be\n///     preferred.\n///\n/// Updates take `O(N)` time. If you need to frequently update weights, consider\n/// [`rand_distr::weighted_tree`](https://docs.rs/rand_distr/*/rand_distr/weighted_tree/index.html)\n/// as an alternative where an update is `O(log N)`.\npub fn update_weights(&mut self, new_weights: &[(usize, &X)]) -> Result<(), Error>\nwhere\n    X: for<'a> core::ops::AddAssign<&'a X>\n        + for<'a> core::ops::SubAssign<&'a X>\n        + Clone\n        + Default,\n{\n    if new_weights.is_empty() {\n        return Ok(());\n    }\n\n    let zero = <X as Default>::default();\n\n    let mut total_weight = self.total_weight.clone();\n\n    // Check for errors first, so we don't modify `self` in case something\n    // goes wrong.\n    let mut prev_i = None;\n    for &(i, w) in new_weights {\n        if let Some(old_i) = prev_i {\n            if old_i >= i {\n                return Err(Error::InvalidInput);\n            }\n        }\n        if !(*w >= zero) {\n            return Err(Error::InvalidWeight);\n        }\n        if i > self.cumulative_weights.len() {\n            return Err(Error::InvalidInput);\n        }\n\n        let mut old_w = if i < self.cumulative_weights.len() {\n            self.cumulative_weights[i].clone()\n        } else {\n            self.total_weight.clone()\n        };\n        if i > 0 {\n            old_w -= &self.cumulative_weights[i - 1];\n        }\n\n        total_weight -= &old_w;\n        total_weight += w;\n        prev_i = Some(i);\n    }\n    if total_weight <= zero {\n        return Err(Error::InsufficientNonZero);\n    }\n\n    // Update the weights. Because we checked all the preconditions in the\n    // previous loop, this should never panic.\n    let mut iter = new_weights.iter();\n\n    let mut prev_weight = zero.clone();\n    let mut next_new_weight = iter.next();\n    let &(first_new_index, _) = next_new_weight.unwrap();\n    let mut cumulative_weight = if first_new_index > 0 {\n        self.cumulative_weights[first_new_index - 1].clone()\n    } else {\n        zero.clone()\n    };\n    for i in first_new_index..self.cumulative_weights.len() {\n        match next_new_weight {\n            Some(&(j, w)) if i == j => {\n                cumulative_weight += w;\n                next_new_weight = iter.next();\n            }\n            _ => {\n                let mut tmp = self.cumulative_weights[i].clone();\n                tmp -= &prev_weight; // We know this is positive.\n                cumulative_weight += &tmp;\n            }\n        }\n        prev_weight = cumulative_weight.clone();\n        core::mem::swap(&mut prev_weight, &mut self.cumulative_weights[i]);\n    }\n\n    self.total_weight = total_weight;\n    self.weight_distribution = X::Sampler::new(zero, self.total_weight.clone()).unwrap();\n\n    Ok(())\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}