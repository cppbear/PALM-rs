{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "// src/engine/general_purpose/mod.rs\n// crate name is base64\nThe function to be tested is presented as follows:\nfn internal_encode(&self, input: &[u8], output: &mut [u8]) -> usize {\n    let mut input_index: usize = 0;\n\n    const BLOCKS_PER_FAST_LOOP: usize = 4;\n    const LOW_SIX_BITS: u64 = 0x3F;\n\n    // we read 8 bytes at a time (u64) but only actually consume 6 of those bytes. Thus, we need\n    // 2 trailing bytes to be available to read..\n    let last_fast_index = input.len().saturating_sub(BLOCKS_PER_FAST_LOOP * 6 + 2);\n    let mut output_index = 0;\n\n    if last_fast_index > 0 {\n        while input_index <= last_fast_index {\n            // Major performance wins from letting the optimizer do the bounds check once, mostly\n            // on the output side\n            let input_chunk =\n                &input[input_index..(input_index + (BLOCKS_PER_FAST_LOOP * 6 + 2))];\n            let output_chunk =\n                &mut output[output_index..(output_index + BLOCKS_PER_FAST_LOOP * 8)];\n\n            // Hand-unrolling for 32 vs 16 or 8 bytes produces yields performance about equivalent\n            // to unsafe pointer code on a Xeon E5-1650v3. 64 byte unrolling was slightly better for\n            // large inputs but significantly worse for 50-byte input, unsurprisingly. I suspect\n            // that it's a not uncommon use case to encode smallish chunks of data (e.g. a 64-byte\n            // SHA-512 digest), so it would be nice if that fit in the unrolled loop at least once.\n            // Plus, single-digit percentage performance differences might well be quite different\n            // on different hardware.\n\n            let input_u64 = read_u64(&input_chunk[0..]);\n\n            output_chunk[0] = self.encode_table[((input_u64 >> 58) & LOW_SIX_BITS) as usize];\n            output_chunk[1] = self.encode_table[((input_u64 >> 52) & LOW_SIX_BITS) as usize];\n            output_chunk[2] = self.encode_table[((input_u64 >> 46) & LOW_SIX_BITS) as usize];\n            output_chunk[3] = self.encode_table[((input_u64 >> 40) & LOW_SIX_BITS) as usize];\n            output_chunk[4] = self.encode_table[((input_u64 >> 34) & LOW_SIX_BITS) as usize];\n            output_chunk[5] = self.encode_table[((input_u64 >> 28) & LOW_SIX_BITS) as usize];\n            output_chunk[6] = self.encode_table[((input_u64 >> 22) & LOW_SIX_BITS) as usize];\n            output_chunk[7] = self.encode_table[((input_u64 >> 16) & LOW_SIX_BITS) as usize];\n\n            let input_u64 = read_u64(&input_chunk[6..]);\n\n            output_chunk[8] = self.encode_table[((input_u64 >> 58) & LOW_SIX_BITS) as usize];\n            output_chunk[9] = self.encode_table[((input_u64 >> 52) & LOW_SIX_BITS) as usize];\n            output_chunk[10] = self.encode_table[((input_u64 >> 46) & LOW_SIX_BITS) as usize];\n            output_chunk[11] = self.encode_table[((input_u64 >> 40) & LOW_SIX_BITS) as usize];\n            output_chunk[12] = self.encode_table[((input_u64 >> 34) & LOW_SIX_BITS) as usize];\n            output_chunk[13] = self.encode_table[((input_u64 >> 28) & LOW_SIX_BITS) as usize];\n            output_chunk[14] = self.encode_table[((input_u64 >> 22) & LOW_SIX_BITS) as usize];\n            output_chunk[15] = self.encode_table[((input_u64 >> 16) & LOW_SIX_BITS) as usize];\n\n            let input_u64 = read_u64(&input_chunk[12..]);\n\n            output_chunk[16] = self.encode_table[((input_u64 >> 58) & LOW_SIX_BITS) as usize];\n            output_chunk[17] = self.encode_table[((input_u64 >> 52) & LOW_SIX_BITS) as usize];\n            output_chunk[18] = self.encode_table[((input_u64 >> 46) & LOW_SIX_BITS) as usize];\n            output_chunk[19] = self.encode_table[((input_u64 >> 40) & LOW_SIX_BITS) as usize];\n            output_chunk[20] = self.encode_table[((input_u64 >> 34) & LOW_SIX_BITS) as usize];\n            output_chunk[21] = self.encode_table[((input_u64 >> 28) & LOW_SIX_BITS) as usize];\n            output_chunk[22] = self.encode_table[((input_u64 >> 22) & LOW_SIX_BITS) as usize];\n            output_chunk[23] = self.encode_table[((input_u64 >> 16) & LOW_SIX_BITS) as usize];\n\n            let input_u64 = read_u64(&input_chunk[18..]);\n\n            output_chunk[24] = self.encode_table[((input_u64 >> 58) & LOW_SIX_BITS) as usize];\n            output_chunk[25] = self.encode_table[((input_u64 >> 52) & LOW_SIX_BITS) as usize];\n            output_chunk[26] = self.encode_table[((input_u64 >> 46) & LOW_SIX_BITS) as usize];\n            output_chunk[27] = self.encode_table[((input_u64 >> 40) & LOW_SIX_BITS) as usize];\n            output_chunk[28] = self.encode_table[((input_u64 >> 34) & LOW_SIX_BITS) as usize];\n            output_chunk[29] = self.encode_table[((input_u64 >> 28) & LOW_SIX_BITS) as usize];\n            output_chunk[30] = self.encode_table[((input_u64 >> 22) & LOW_SIX_BITS) as usize];\n            output_chunk[31] = self.encode_table[((input_u64 >> 16) & LOW_SIX_BITS) as usize];\n\n            output_index += BLOCKS_PER_FAST_LOOP * 8;\n            input_index += BLOCKS_PER_FAST_LOOP * 6;\n        }\n    }\n\n    // Encode what's left after the fast loop.\n\n    const LOW_SIX_BITS_U8: u8 = 0x3F;\n\n    let rem = input.len() % 3;\n    let start_of_rem = input.len() - rem;\n\n    // start at the first index not handled by fast loop, which may be 0.\n\n    while input_index < start_of_rem {\n        let input_chunk = &input[input_index..(input_index + 3)];\n        let output_chunk = &mut output[output_index..(output_index + 4)];\n\n        output_chunk[0] = self.encode_table[(input_chunk[0] >> 2) as usize];\n        output_chunk[1] = self.encode_table\n            [((input_chunk[0] << 4 | input_chunk[1] >> 4) & LOW_SIX_BITS_U8) as usize];\n        output_chunk[2] = self.encode_table\n            [((input_chunk[1] << 2 | input_chunk[2] >> 6) & LOW_SIX_BITS_U8) as usize];\n        output_chunk[3] = self.encode_table[(input_chunk[2] & LOW_SIX_BITS_U8) as usize];\n\n        input_index += 3;\n        output_index += 4;\n    }\n\n    if rem == 2 {\n        output[output_index] = self.encode_table[(input[start_of_rem] >> 2) as usize];\n        output[output_index + 1] =\n            self.encode_table[((input[start_of_rem] << 4 | input[start_of_rem + 1] >> 4)\n                & LOW_SIX_BITS_U8) as usize];\n        output[output_index + 2] =\n            self.encode_table[((input[start_of_rem + 1] << 2) & LOW_SIX_BITS_U8) as usize];\n        output_index += 3;\n    } else if rem == 1 {\n        output[output_index] = self.encode_table[(input[start_of_rem] >> 2) as usize];\n        output[output_index + 1] =\n            self.encode_table[((input[start_of_rem] << 4) & LOW_SIX_BITS_U8) as usize];\n        output_index += 2;\n    }\n\n    output_index\n}\n",
  "depend_pt": ""
}