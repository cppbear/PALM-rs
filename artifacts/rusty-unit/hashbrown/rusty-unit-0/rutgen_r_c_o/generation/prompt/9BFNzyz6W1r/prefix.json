{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. Omit test oracles and assertions; concentrate on generating test inputs and function calls of the focal function; do not use \"_\" for the return values of the focal function.\n3. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides.\n6. If the method under test uses generics, instantiate them with suitable types based on the context.\n7. Define any necessary helper structures or implementations directly within the test function when required.\n8. Create a minimal yet comprehensive set of test functions, ensuring each test input satisfies all given constraints, with some explicitly covering edge scenarios.\n9. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\npub struct Bucket<T> {\n    ptr: NonNull<T>,\n}\nimpl<T> Bucket<T> {\n    #[inline]\n    unsafe fn from_base_index(base: NonNull<T>, index: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(index + 1)\n        } else {\n            base.as_ptr().sub(index)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[inline]\n    unsafe fn to_base_index(&self, base: NonNull<T>) -> usize {\n        if T::IS_ZERO_SIZED {\n            self.ptr.as_ptr() as usize - 1\n        } else {\n            offset_from(base.as_ptr(), self.ptr.as_ptr())\n        }\n    }\n    #[inline]\n    pub fn as_ptr(&self) -> *mut T {}\n    #[inline]\n    fn as_non_null(&self) -> NonNull<T> {}\n    #[inline]\n    unsafe fn next_n(&self, offset: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(self.ptr.as_ptr() as usize + offset)\n        } else {\n            self.ptr.as_ptr().sub(offset)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    pub(crate) unsafe fn drop(&self) {}\n    #[inline]\n    pub(crate) unsafe fn read(&self) -> T {}\n    #[inline]\n    pub(crate) unsafe fn write(&self, val: T) {}\n    #[inline]\n    pub unsafe fn as_ref<'a>(&self) -> &'a T {}\n    #[inline]\n    pub unsafe fn as_mut<'a>(&self) -> &'a mut T {}\n}\n#[inline]\nunsafe fn offset_from<T>(to: *const T, from: *const T) -> usize {\n    to.offset_from(from) as usize\n}\n\nThe function to be tested is presented as follows:\n/// Calculates the index of a [`Bucket`] as distance between two pointers\n/// (convenience for `base.as_ptr().offset_from(self.ptr.as_ptr()) as usize`).\n/// The returned value is in units of T: the distance in bytes divided by\n/// [`core::mem::size_of::<T>()`].\n///\n/// If the `T` is a ZST, then we return the index of the element in\n/// the table so that `erase` works properly (return `self.ptr.as_ptr() as usize - 1`).\n///\n/// This function is the inverse of [`from_base_index`].\n///\n/// # Safety\n///\n/// If `mem::size_of::<T>() != 0`, then the safety rules are directly derived\n/// from the safety rules for [`<*const T>::offset_from`] method of `*const T`.\n///\n/// Thus, in order to uphold the safety contracts for [`<*const T>::offset_from`]\n/// method, as well as for the correct logic of the work of this crate, the\n/// following rules are necessary and sufficient:\n///\n/// * `base` contained pointer must not be `dangling` and must point to the\n///   end of the first `element` from the `data part` of the table, i.e.\n///   must be a pointer that returns by [`RawTable::data_end`] or by\n///   [`RawTableInner::data_end<T>`];\n///\n/// * `self` also must not contain dangling pointer;\n///\n/// * both `self` and `base` must be created from the same [`RawTable`]\n///   (or [`RawTableInner`]).\n///\n/// If `mem::size_of::<T>() == 0`, this function is always safe.\n///\n/// [`Bucket`]: crate::raw::Bucket\n/// [`from_base_index`]: crate::raw::Bucket::from_base_index\n/// [`RawTable::data_end`]: crate::raw::RawTable::data_end\n/// [`RawTableInner::data_end<T>`]: RawTableInner::data_end<T>\n/// [`RawTable`]: crate::raw::RawTable\n/// [`RawTableInner`]: RawTableInner\n/// [`<*const T>::offset_from`]: https://doc.rust-lang.org/nightly/core/primitive.pointer.html#method.offset_from\nunsafe fn to_base_index(&self, base: NonNull<T>) -> usize {\n    // If mem::size_of::<T>() != 0 then return an index under which we used to store the\n    // `element` in the data part of the table (we start counting from \"0\", so\n    // that in the expression T[last], the \"last\" index actually is one less than the\n    // \"buckets\" number in the table, i.e. \"last = RawTableInner.bucket_mask\").\n    // For example for 5th element in table calculation is performed like this:\n    //\n    //                        mem::size_of::<T>()\n    //                          |\n    //                          |         `self = from_base_index(base, 5)` that returns pointer\n    //                          |         that points here in the data part of the table\n    //                          |         (to the end of T5)\n    //                          |           |                    `base: NonNull<T>` must point here\n    //                          v           |                    (to the end of T0 or to the start of C0)\n    //                        /???\\         v                      v\n    // [Padding], Tlast, ..., |T10|, ..., T5|, T4, T3, T2, T1, T0, |C0, C1, C2, C3, C4, C5, ..., C10, ..., Clast\n    //                                      \\__________  __________/\n    //                                                 \\/\n    //                                     `bucket.to_base_index(base)` = 5\n    //                                     (base.as_ptr() as usize - self.ptr.as_ptr() as usize) / mem::size_of::<T>()\n    //\n    // where: T0...Tlast - our stored data; C0...Clast - control bytes or metadata for data.\n    if T::IS_ZERO_SIZED {\n        // this can not be UB\n        self.ptr.as_ptr() as usize - 1\n    } else {\n        offset_from(base.as_ptr(), self.ptr.as_ptr())\n    }\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}