{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. Omit test oracles and assertions; concentrate on generating test inputs and function calls of the focal function; do not use \"_\" for the return values of the focal function.\n3. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides.\n6. If the method under test uses generics, instantiate them with suitable types based on the context.\n7. Define any necessary helper structures or implementations directly within the test function when required.\n8. Create a minimal yet comprehensive set of test functions, ensuring each test input satisfies all given constraints, with some explicitly covering edge scenarios.\n9. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\npub struct InsertSlot {\n    index: usize,\n}\n#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n#[repr(transparent)]\npub(crate) struct Tag(u8);\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl(0).write_bytes(Tag::EMPTY.0, result.num_ctrl_bytes());\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {\n        if unlikely(self.is_bucket_full(index)) {\n            debug_assert!(self.bucket_mask < Group::WIDTH);\n            index = Group::load_aligned(self.ctrl(0))\n                .match_empty_or_deleted()\n                .lowest_set_bit()\n                .unwrap_unchecked();\n        }\n        InsertSlot { index }\n    }\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {}\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {}\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {}\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {\n        debug_assert!(index < self.num_ctrl_bytes());\n        self.ctrl.as_ptr().add(index).cast()\n    }\n    #[inline]\n    fn buckets(&self) -> usize {}\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {\n        debug_assert!(index < self.buckets());\n        (*self.ctrl(index)).is_full()\n    }\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {}\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {}\n}\n#[allow(clippy::use_self)]\nimpl BitMask {\n    #[inline]\n    #[must_use]\n    #[allow(dead_code)]\n    pub(crate) fn invert(self) -> Self {\n        BitMask(self.0 ^ BITMASK_MASK)\n    }\n    #[inline]\n    #[must_use]\n    fn remove_lowest_bit(self) -> Self {\n        BitMask(self.0 & (self.0 - 1))\n    }\n    #[inline]\n    pub(crate) fn any_bit_set(self) -> bool {}\n    #[inline]\n    pub(crate) fn lowest_set_bit(self) -> Option<usize> {\n        if let Some(nonzero) = NonZeroBitMaskWord::new(self.0) {\n            Some(Self::nonzero_trailing_zeros(nonzero))\n        } else {\n            None\n        }\n    }\n    #[inline]\n    pub(crate) fn trailing_zeros(self) -> usize {}\n    #[inline]\n    fn nonzero_trailing_zeros(nonzero: NonZeroBitMaskWord) -> usize {}\n    #[inline]\n    pub(crate) fn leading_zeros(self) -> usize {}\n}\n\nThe function to be tested is presented as follows:\n/// Fixes up an insertion slot returned by the [`RawTableInner::find_insert_slot_in_group`] method.\n///\n/// In tables smaller than the group width (`self.buckets() < Group::WIDTH`), trailing control\n/// bytes outside the range of the table are filled with [`Tag::EMPTY`] entries. These will unfortunately\n/// trigger a match of [`RawTableInner::find_insert_slot_in_group`] function. This is because\n/// the `Some(bit)` returned by `group.match_empty_or_deleted().lowest_set_bit()` after masking\n/// (`(probe_seq.pos + bit) & self.bucket_mask`) may point to a full bucket that is already occupied.\n/// We detect this situation here and perform a second scan starting at the beginning of the table.\n/// This second scan is guaranteed to find an empty slot (due to the load factor) before hitting the\n/// trailing control bytes (containing [`Tag::EMPTY`] bytes).\n///\n/// If this function is called correctly, it is guaranteed to return [`InsertSlot`] with an\n/// index of an empty or deleted bucket in the range `0..self.buckets()` (see `Warning` and\n/// `Safety`).\n///\n/// # Warning\n///\n/// The table must have at least 1 empty or deleted `bucket`, otherwise if the table is less than\n/// the group width (`self.buckets() < Group::WIDTH`) this function returns an index outside of the\n/// table indices range `0..self.buckets()` (`0..=self.bucket_mask`). Attempt to write data at that\n/// index will cause immediate [`undefined behavior`].\n///\n/// # Safety\n///\n/// The safety rules are directly derived from the safety rules for [`RawTableInner::ctrl`] method.\n/// Thus, in order to uphold those safety contracts, as well as for the correct logic of the work\n/// of this crate, the following rules are necessary and sufficient:\n///\n/// * The [`RawTableInner`] must have properly initialized control bytes otherwise calling this\n///   function results in [`undefined behavior`].\n///\n/// * This function must only be used on insertion slots found by [`RawTableInner::find_insert_slot_in_group`]\n///   (after the `find_insert_slot_in_group` function, but before insertion into the table).\n///\n/// * The `index` must not be greater than the `self.bucket_mask`, i.e. `(index + 1) <= self.buckets()`\n///   (this one is provided by the [`RawTableInner::find_insert_slot_in_group`] function).\n///\n/// Calling this function with an index not provided by [`RawTableInner::find_insert_slot_in_group`]\n/// may result in [`undefined behavior`] even if the index satisfies the safety rules of the\n/// [`RawTableInner::ctrl`] function (`index < self.bucket_mask + 1 + Group::WIDTH`).\n///\n/// [`RawTableInner::ctrl`]: RawTableInner::ctrl\n/// [`RawTableInner::find_insert_slot_in_group`]: RawTableInner::find_insert_slot_in_group\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\nunsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {\n    // SAFETY: The caller of this function ensures that `index` is in the range `0..=self.bucket_mask`.\n    if unlikely(self.is_bucket_full(index)) {\n        debug_assert!(self.bucket_mask < Group::WIDTH);\n        // SAFETY:\n        //\n        // * Since the caller of this function ensures that the control bytes are properly\n        //   initialized and `ptr = self.ctrl(0)` points to the start of the array of control\n        //   bytes, therefore: `ctrl` is valid for reads, properly aligned to `Group::WIDTH`\n        //   and points to the properly initialized control bytes (see also\n        //   `TableLayout::calculate_layout_for` and `ptr::read`);\n        //\n        // * Because the caller of this function ensures that the index was provided by the\n        //   `self.find_insert_slot_in_group()` function, so for for tables larger than the\n        //   group width (self.buckets() >= Group::WIDTH), we will never end up in the given\n        //   branch, since `(probe_seq.pos + bit) & self.bucket_mask` in `find_insert_slot_in_group`\n        //   cannot return a full bucket index. For tables smaller than the group width, calling\n        //   the `unwrap_unchecked` function is also safe, as the trailing control bytes outside\n        //   the range of the table are filled with EMPTY bytes (and we know for sure that there\n        //   is at least one FULL bucket), so this second scan either finds an empty slot (due to\n        //   the load factor) or hits the trailing control bytes (containing EMPTY).\n        index = Group::load_aligned(self.ctrl(0))\n            .match_empty_or_deleted()\n            .lowest_set_bit()\n            .unwrap_unchecked();\n    }\n    InsertSlot { index }\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}