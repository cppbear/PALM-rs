{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\n#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n#[repr(transparent)]\npub(crate) struct Tag(u8);\npub struct InsertSlot {\n    index: usize,\n}\npub struct ScopeGuard<T, F>\nwhere\n    F: FnMut(&mut T),\n{\n    dropfn: F,\n    value: T,\n}\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl(0).write_bytes(Tag::EMPTY.0, result.num_ctrl_bytes());\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {}\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {}\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {\n        let mut probe_seq = self.probe_seq(hash);\n        loop {\n            let group = unsafe { Group::load(self.ctrl(probe_seq.pos)) };\n            let index = self.find_insert_slot_in_group(&group, &probe_seq);\n            if likely(index.is_some()) {\n                unsafe {\n                    return self.fix_insert_slot(index.unwrap_unchecked());\n                }\n            }\n            probe_seq.move_next(self.bucket_mask);\n        }\n    }\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {\n        for i in (0..self.buckets()).step_by(Group::WIDTH) {\n            let group = Group::load_aligned(self.ctrl(i));\n            let group = group.convert_special_to_empty_and_full_to_deleted();\n            group.store_aligned(self.ctrl(i));\n        }\n        if unlikely(self.buckets() < Group::WIDTH) {\n            self.ctrl(0).copy_to(self.ctrl(Group::WIDTH), self.buckets());\n        } else {\n            self.ctrl(0).copy_to(self.ctrl(self.buckets()), Group::WIDTH);\n        }\n    }\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {\n        debug_assert_ne!(self.bucket_mask, 0);\n        debug_assert!(index < self.buckets());\n        let base: *mut u8 = self.data_end().as_ptr();\n        base.sub((index + 1) * size_of)\n    }\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {}\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {\n        let probe_seq_pos = self.probe_seq(hash).pos;\n        let probe_index = |pos: usize| {\n            (pos.wrapping_sub(probe_seq_pos) & self.bucket_mask) / Group::WIDTH\n        };\n        probe_index(i) == probe_index(new_i)\n    }\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {\n        self.set_ctrl(index, Tag::full(hash));\n    }\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {\n        let prev_ctrl = *self.ctrl(index);\n        self.set_ctrl_hash(index, hash);\n        prev_ctrl\n    }\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {\n        let index2 = ((index.wrapping_sub(Group::WIDTH)) & self.bucket_mask)\n            + Group::WIDTH;\n        *self.ctrl(index) = ctrl;\n        *self.ctrl(index2) = ctrl;\n    }\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {\n        debug_assert!(index < self.num_ctrl_bytes());\n        self.ctrl.as_ptr().add(index).cast()\n    }\n    #[inline]\n    fn buckets(&self) -> usize {\n        self.bucket_mask + 1\n    }\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {}\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {}\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {\n        self.prepare_rehash_in_place();\n        let mut guard = guard(\n            self,\n            move |self_| {\n                if let Some(drop) = drop {\n                    for i in 0..self_.buckets() {\n                        if *self_.ctrl(i) == Tag::DELETED {\n                            self_.set_ctrl(i, Tag::EMPTY);\n                            drop(self_.bucket_ptr(i, size_of));\n                            self_.items -= 1;\n                        }\n                    }\n                }\n                self_.growth_left = bucket_mask_to_capacity(self_.bucket_mask)\n                    - self_.items;\n            },\n        );\n        'outer: for i in 0..guard.buckets() {\n            if *guard.ctrl(i) != Tag::DELETED {\n                continue;\n            }\n            let i_p = guard.bucket_ptr(i, size_of);\n            'inner: loop {\n                let hash = hasher(*guard, i);\n                let new_i = guard.find_insert_slot(hash).index;\n                if likely(guard.is_in_same_group(i, new_i, hash)) {\n                    guard.set_ctrl_hash(i, hash);\n                    continue 'outer;\n                }\n                let new_i_p = guard.bucket_ptr(new_i, size_of);\n                let prev_ctrl = guard.replace_ctrl_hash(new_i, hash);\n                if prev_ctrl == Tag::EMPTY {\n                    guard.set_ctrl(i, Tag::EMPTY);\n                    ptr::copy_nonoverlapping(i_p, new_i_p, size_of);\n                    continue 'outer;\n                } else {\n                    debug_assert_eq!(prev_ctrl, Tag::DELETED);\n                    ptr::swap_nonoverlapping(i_p, new_i_p, size_of);\n                    continue 'inner;\n                }\n            }\n        }\n        guard.growth_left = bucket_mask_to_capacity(guard.bucket_mask) - guard.items;\n        mem::forget(guard);\n    }\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {}\n}\n#[inline]\npub fn guard<T, F>(value: T, dropfn: F) -> ScopeGuard<T, F>\nwhere\n    F: FnMut(&mut T),\n{\n    ScopeGuard { dropfn, value }\n}\n#[inline]\nfn bucket_mask_to_capacity(bucket_mask: usize) -> usize {\n    if bucket_mask < 8 { bucket_mask } else { ((bucket_mask + 1) / 8) * 7 }\n}\n\nThe function to be tested is presented as follows:\n/// Rehashes the contents of the table in place (i.e. without changing the\n/// allocation).\n///\n/// If `hasher` panics then some the table's contents may be lost.\n///\n/// This uses dynamic dispatch to reduce the amount of\n/// code generated, but it is eliminated by LLVM optimizations when inlined.\n///\n/// # Safety\n///\n/// If any of the following conditions are violated, the result is [`undefined behavior`]:\n///\n/// * The `size_of` must be equal to the size of the elements stored in the table;\n///\n/// * The `drop` function (`fn(*mut u8)`) must be the actual drop function of\n///   the elements stored in the table.\n///\n/// * The [`RawTableInner`] has already been allocated;\n///\n/// * The [`RawTableInner`] must have properly initialized control bytes.\n///\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\nunsafe fn rehash_in_place(\n    &mut self,\n    hasher: &dyn Fn(&mut Self, usize) -> u64,\n    size_of: usize,\n    drop: Option<unsafe fn(*mut u8)>,\n) {\n    // If the hash function panics then properly clean up any elements\n    // that we haven't rehashed yet. We unfortunately can't preserve the\n    // element since we lost their hash and have no way of recovering it\n    // without risking another panic.\n    self.prepare_rehash_in_place();\n\n    let mut guard = guard(self, move |self_| {\n        if let Some(drop) = drop {\n            for i in 0..self_.buckets() {\n                if *self_.ctrl(i) == Tag::DELETED {\n                    self_.set_ctrl(i, Tag::EMPTY);\n                    drop(self_.bucket_ptr(i, size_of));\n                    self_.items -= 1;\n                }\n            }\n        }\n        self_.growth_left = bucket_mask_to_capacity(self_.bucket_mask) - self_.items;\n    });\n\n    // At this point, DELETED elements are elements that we haven't\n    // rehashed yet. Find them and re-insert them at their ideal\n    // position.\n    'outer: for i in 0..guard.buckets() {\n        if *guard.ctrl(i) != Tag::DELETED {\n            continue;\n        }\n\n        let i_p = guard.bucket_ptr(i, size_of);\n\n        'inner: loop {\n            // Hash the current item\n            let hash = hasher(*guard, i);\n\n            // Search for a suitable place to put it\n            //\n            // SAFETY: Caller of this function ensures that the control bytes\n            // are properly initialized.\n            let new_i = guard.find_insert_slot(hash).index;\n\n            // Probing works by scanning through all of the control\n            // bytes in groups, which may not be aligned to the group\n            // size. If both the new and old position fall within the\n            // same unaligned group, then there is no benefit in moving\n            // it and we can just continue to the next item.\n            if likely(guard.is_in_same_group(i, new_i, hash)) {\n                guard.set_ctrl_hash(i, hash);\n                continue 'outer;\n            }\n\n            let new_i_p = guard.bucket_ptr(new_i, size_of);\n\n            // We are moving the current item to a new position. Write\n            // our H2 to the control byte of the new position.\n            let prev_ctrl = guard.replace_ctrl_hash(new_i, hash);\n            if prev_ctrl == Tag::EMPTY {\n                guard.set_ctrl(i, Tag::EMPTY);\n                // If the target slot is empty, simply move the current\n                // element into the new slot and clear the old control\n                // byte.\n                ptr::copy_nonoverlapping(i_p, new_i_p, size_of);\n                continue 'outer;\n            } else {\n                // If the target slot is occupied, swap the two elements\n                // and then continue processing the element that we just\n                // swapped into the old slot.\n                debug_assert_eq!(prev_ctrl, Tag::DELETED);\n                ptr::swap_nonoverlapping(i_p, new_i_p, size_of);\n                continue 'inner;\n            }\n        }\n    }\n\n    guard.growth_left = bucket_mask_to_capacity(guard.bucket_mask) - guard.items;\n\n    mem::forget(guard);\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}