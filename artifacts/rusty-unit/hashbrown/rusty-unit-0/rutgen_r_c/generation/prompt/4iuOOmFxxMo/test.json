{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\nstruct RawTableInner {\n    bucket_mask: usize,\n    ctrl: NonNull<u8>,\n    growth_left: usize,\n    items: usize,\n}\npub(crate) struct FullBucketsIndices {\n    current_group: BitMaskIter,\n    group_first_index: usize,\n    ctrl: NonNull<u8>,\n    items: usize,\n}\n#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n#[repr(transparent)]\npub(crate) struct Tag(u8);\n#[derive(Copy, Clone)]\nstruct TableLayout {\n    size: usize,\n    ctrl_align: usize,\n}\n#[derive(Copy, Clone)]\nenum Fallibility {\n    Fallible,\n    Infallible,\n}\n#[derive(Clone, PartialEq, Eq, Debug)]\npub enum TryReserveError {\n    /// Error due to the computed capacity exceeding the collection's maximum\n    /// (usually `isize::MAX` bytes).\n    CapacityOverflow,\n    /// The memory allocator returned an error\n    AllocError {\n        /// The layout of the allocation request that failed.\n        layout: alloc::alloc::Layout,\n    },\n}\nimpl RawTableInner {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new_uninitialized<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        buckets: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        debug_assert!(buckets.is_power_of_two());\n        let (layout, ctrl_offset) = match table_layout.calculate_layout_for(buckets) {\n            Some(lco) => lco,\n            None => return Err(fallibility.capacity_overflow()),\n        };\n        let ptr: NonNull<u8> = match do_alloc(alloc, layout) {\n            Ok(block) => block.cast(),\n            Err(_) => return Err(fallibility.alloc_err(layout)),\n        };\n        let ctrl = NonNull::new_unchecked(ptr.as_ptr().add(ctrl_offset));\n        Ok(Self {\n            ctrl,\n            bucket_mask: buckets - 1,\n            items: 0,\n            growth_left: bucket_mask_to_capacity(buckets - 1),\n        })\n    }\n    #[inline]\n    fn fallible_with_capacity<A>(\n        alloc: &A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<Self, TryReserveError>\n    where\n        A: Allocator,\n    {\n        if capacity == 0 {\n            Ok(Self::NEW)\n        } else {\n            unsafe {\n                let buckets = capacity_to_buckets(capacity)\n                    .ok_or_else(|| fallibility.capacity_overflow())?;\n                let result = Self::new_uninitialized(\n                    alloc,\n                    table_layout,\n                    buckets,\n                    fallibility,\n                )?;\n                result.ctrl(0).write_bytes(Tag::EMPTY.0, result.num_ctrl_bytes());\n                Ok(result)\n            }\n        }\n    }\n    fn with_capacity<A>(alloc: &A, table_layout: TableLayout, capacity: usize) -> Self\n    where\n        A: Allocator,\n    {\n        match Self::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            Fallibility::Infallible,\n        ) {\n            Ok(table_inner) => table_inner,\n            Err(_) => unsafe { hint::unreachable_unchecked() }\n        }\n    }\n    #[inline]\n    unsafe fn fix_insert_slot(&self, mut index: usize) -> InsertSlot {}\n    #[inline]\n    fn find_insert_slot_in_group(\n        &self,\n        group: &Group,\n        probe_seq: &ProbeSeq,\n    ) -> Option<usize> {}\n    #[inline]\n    unsafe fn find_or_find_insert_slot_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Result<usize, InsertSlot> {}\n    #[inline]\n    unsafe fn prepare_insert_slot(&mut self, hash: u64) -> (usize, Tag) {\n        let index: usize = self.find_insert_slot(hash).index;\n        let old_ctrl = *self.ctrl(index);\n        self.set_ctrl_hash(index, hash);\n        (index, old_ctrl)\n    }\n    #[inline]\n    unsafe fn find_insert_slot(&self, hash: u64) -> InsertSlot {}\n    #[inline(always)]\n    unsafe fn find_inner(\n        &self,\n        hash: u64,\n        eq: &mut dyn FnMut(usize) -> bool,\n    ) -> Option<usize> {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    unsafe fn prepare_rehash_in_place(&mut self) {}\n    #[inline]\n    unsafe fn iter<T>(&self) -> RawIter<T> {}\n    unsafe fn drop_elements<T>(&mut self) {}\n    unsafe fn drop_inner_table<T, A: Allocator>(\n        &mut self,\n        alloc: &A,\n        table_layout: TableLayout,\n    ) {}\n    #[inline]\n    unsafe fn bucket<T>(&self, index: usize) -> Bucket<T> {}\n    #[inline]\n    unsafe fn bucket_ptr(&self, index: usize, size_of: usize) -> *mut u8 {\n        debug_assert_ne!(self.bucket_mask, 0);\n        debug_assert!(index < self.buckets());\n        let base: *mut u8 = self.data_end().as_ptr();\n        base.sub((index + 1) * size_of)\n    }\n    #[inline]\n    fn data_end<T>(&self) -> NonNull<T> {}\n    #[inline]\n    fn probe_seq(&self, hash: u64) -> ProbeSeq {}\n    #[inline]\n    unsafe fn record_item_insert_at(&mut self, index: usize, old_ctrl: Tag, hash: u64) {}\n    #[inline]\n    fn is_in_same_group(&self, i: usize, new_i: usize, hash: u64) -> bool {}\n    #[inline]\n    unsafe fn set_ctrl_hash(&mut self, index: usize, hash: u64) {}\n    #[inline]\n    unsafe fn replace_ctrl_hash(&mut self, index: usize, hash: u64) -> Tag {}\n    #[inline]\n    unsafe fn set_ctrl(&mut self, index: usize, ctrl: Tag) {}\n    #[inline]\n    unsafe fn ctrl(&self, index: usize) -> *mut Tag {}\n    #[inline]\n    fn buckets(&self) -> usize {}\n    #[inline]\n    unsafe fn is_bucket_full(&self, index: usize) -> bool {}\n    #[inline]\n    fn num_ctrl_bytes(&self) -> usize {}\n    #[inline]\n    fn is_empty_singleton(&self) -> bool {}\n    #[allow(clippy::mut_mut)]\n    #[inline]\n    fn prepare_resize<'a, A>(\n        &self,\n        alloc: &'a A,\n        table_layout: TableLayout,\n        capacity: usize,\n        fallibility: Fallibility,\n    ) -> Result<\n        crate::scopeguard::ScopeGuard<Self, impl FnMut(&mut Self) + 'a>,\n        TryReserveError,\n    >\n    where\n        A: Allocator,\n    {\n        debug_assert!(self.items <= capacity);\n        let new_table = RawTableInner::fallible_with_capacity(\n            alloc,\n            table_layout,\n            capacity,\n            fallibility,\n        )?;\n        Ok(\n            guard(\n                new_table,\n                move |self_| {\n                    if !self_.is_empty_singleton() {\n                        unsafe { self_.free_buckets(alloc, table_layout) };\n                    }\n                },\n            ),\n        )\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn reserve_rehash_inner<A>(\n        &mut self,\n        alloc: &A,\n        additional: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {}\n    #[inline(always)]\n    unsafe fn full_buckets_indices(&self) -> FullBucketsIndices {\n        let ctrl = NonNull::new_unchecked(self.ctrl(0).cast::<u8>());\n        FullBucketsIndices {\n            current_group: Group::load_aligned(ctrl.as_ptr().cast())\n                .match_full()\n                .into_iter(),\n            group_first_index: 0,\n            ctrl,\n            items: self.items,\n        }\n    }\n    #[allow(clippy::inline_always)]\n    #[inline(always)]\n    unsafe fn resize_inner<A>(\n        &mut self,\n        alloc: &A,\n        capacity: usize,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        fallibility: Fallibility,\n        layout: TableLayout,\n    ) -> Result<(), TryReserveError>\n    where\n        A: Allocator,\n    {\n        let mut new_table = self.prepare_resize(alloc, layout, capacity, fallibility)?;\n        for full_byte_index in self.full_buckets_indices() {\n            let hash = hasher(self, full_byte_index);\n            let (new_index, _) = new_table.prepare_insert_slot(hash);\n            ptr::copy_nonoverlapping(\n                self.bucket_ptr(full_byte_index, layout.size),\n                new_table.bucket_ptr(new_index, layout.size),\n                layout.size,\n            );\n        }\n        new_table.growth_left -= self.items;\n        new_table.items = self.items;\n        mem::swap(self, &mut new_table);\n        Ok(())\n    }\n    #[allow(clippy::inline_always)]\n    #[cfg_attr(feature = \"inline-more\", inline(always))]\n    #[cfg_attr(not(feature = \"inline-more\"), inline)]\n    unsafe fn rehash_in_place(\n        &mut self,\n        hasher: &dyn Fn(&mut Self, usize) -> u64,\n        size_of: usize,\n        drop: Option<unsafe fn(*mut u8)>,\n    ) {}\n    #[inline]\n    unsafe fn free_buckets<A>(&mut self, alloc: &A, table_layout: TableLayout)\n    where\n        A: Allocator,\n    {}\n    #[inline]\n    unsafe fn allocation_info(\n        &self,\n        table_layout: TableLayout,\n    ) -> (NonNull<u8>, Layout) {}\n    #[inline]\n    unsafe fn allocation_size_or_zero(&self, table_layout: TableLayout) -> usize {}\n    #[inline]\n    fn clear_no_drop(&mut self) {}\n    #[inline]\n    unsafe fn erase(&mut self, index: usize) {}\n}\nimpl Iterator for FullBucketsIndices {\n    type Item = usize;\n    #[inline(always)]\n    fn next(&mut self) -> Option<usize> {\n        if self.items == 0 {\n            return None;\n        }\n        let nxt = unsafe { self.next_impl() };\n        debug_assert!(nxt.is_some());\n        self.items -= 1;\n        nxt\n    }\n    #[inline(always)]\n    fn size_hint(&self) -> (usize, Option<usize>) {}\n}\n\nThe function to be tested is presented as follows:\n/// Allocates a new table of a different size and moves the contents of the\n/// current table into it.\n///\n/// This uses dynamic dispatch to reduce the amount of\n/// code generated, but it is eliminated by LLVM optimizations when inlined.\n///\n/// # Safety\n///\n/// If any of the following conditions are violated, the result is\n/// [`undefined behavior`]:\n///\n/// * The `alloc` must be the same [`Allocator`] as the `Allocator` used\n///   to allocate this table;\n///\n/// * The `layout` must be the same [`TableLayout`] as the `TableLayout`\n///   used to allocate this table;\n///\n/// * The [`RawTableInner`] must have properly initialized control bytes.\n///\n/// The caller of this function must ensure that `capacity >= self.items`\n/// otherwise:\n///\n/// * If `self.items != 0`, calling of this function with `capacity == 0`\n///   results in [`undefined behavior`].\n///\n/// * If `capacity_to_buckets(capacity) < Group::WIDTH` and\n///   `self.items > capacity_to_buckets(capacity)` calling this function\n///   results in [`undefined behavior`].\n///\n/// * If `capacity_to_buckets(capacity) >= Group::WIDTH` and\n///   `self.items > capacity_to_buckets(capacity)` calling this function\n///   are never return (will go into an infinite loop).\n///\n/// Note: It is recommended (but not required) that the new table's `capacity`\n/// be greater than or equal to `self.items`. In case if `capacity <= self.items`\n/// this function can never return. See [`RawTableInner::find_insert_slot`] for\n/// more information.\n///\n/// [`RawTableInner::find_insert_slot`]: RawTableInner::find_insert_slot\n/// [`undefined behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\nunsafe fn resize_inner<A>(\n    &mut self,\n    alloc: &A,\n    capacity: usize,\n    hasher: &dyn Fn(&mut Self, usize) -> u64,\n    fallibility: Fallibility,\n    layout: TableLayout,\n) -> Result<(), TryReserveError>\nwhere\n    A: Allocator,\n{\n    // SAFETY: We know for sure that `alloc` and `layout` matches the [`Allocator`] and [`TableLayout`]\n    // that were used to allocate this table.\n    let mut new_table = self.prepare_resize(alloc, layout, capacity, fallibility)?;\n\n    // SAFETY: We know for sure that RawTableInner will outlive the\n    // returned `FullBucketsIndices` iterator, and the caller of this\n    // function ensures that the control bytes are properly initialized.\n    for full_byte_index in self.full_buckets_indices() {\n        // This may panic.\n        let hash = hasher(self, full_byte_index);\n\n        // SAFETY:\n        // We can use a simpler version of insert() here since:\n        // 1. There are no DELETED entries.\n        // 2. We know there is enough space in the table.\n        // 3. All elements are unique.\n        // 4. The caller of this function guarantees that `capacity > 0`\n        //    so `new_table` must already have some allocated memory.\n        // 5. We set `growth_left` and `items` fields of the new table\n        //    after the loop.\n        // 6. We insert into the table, at the returned index, the data\n        //    matching the given hash immediately after calling this function.\n        let (new_index, _) = new_table.prepare_insert_slot(hash);\n\n        // SAFETY:\n        //\n        // * `src` is valid for reads of `layout.size` bytes, since the\n        //   table is alive and the `full_byte_index` is guaranteed to be\n        //   within bounds (see `FullBucketsIndices::next_impl`);\n        //\n        // * `dst` is valid for writes of `layout.size` bytes, since the\n        //   caller ensures that `table_layout` matches the [`TableLayout`]\n        //   that was used to allocate old table and we have the `new_index`\n        //   returned by `prepare_insert_slot`.\n        //\n        // * Both `src` and `dst` are properly aligned.\n        //\n        // * Both `src` and `dst` point to different region of memory.\n        ptr::copy_nonoverlapping(\n            self.bucket_ptr(full_byte_index, layout.size),\n            new_table.bucket_ptr(new_index, layout.size),\n            layout.size,\n        );\n    }\n\n    // The hash function didn't panic, so we can safely set the\n    // `growth_left` and `items` fields of the new table.\n    new_table.growth_left -= self.items;\n    new_table.items = self.items;\n\n    // We successfully copied all elements without panicking. Now replace\n    // self with the new table. The old table will have its memory freed but\n    // the items will not be dropped (since they have been moved into the\n    // new table).\n    // SAFETY: The caller ensures that `table_layout` matches the [`TableLayout`]\n    // that was used to allocate this table.\n    mem::swap(self, &mut new_table);\n\n    Ok(())\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}