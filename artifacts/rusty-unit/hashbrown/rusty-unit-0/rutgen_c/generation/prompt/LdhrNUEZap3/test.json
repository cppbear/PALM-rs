{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/raw/mod.rs\n// crate name is hashbrown\nuse crate::alloc::alloc::{handle_alloc_error, Layout};\nuse crate::scopeguard::{guard, ScopeGuard};\nuse crate::TryReserveError;\nuse core::array;\nuse core::iter::FusedIterator;\nuse core::marker::PhantomData;\nuse core::mem;\nuse core::ptr::NonNull;\nuse core::{hint, ptr};\npub(crate) use self::alloc::{do_alloc, Allocator, Global};\nuse self::bitmask::BitMaskIter;\nuse self::imp::Group;\n#[cfg(not(feature = \"nightly\"))]\nuse core::convert::{identity as likely, identity as unlikely};\n#[cfg(feature = \"nightly\")]\nuse core::intrinsics::{likely, unlikely};\ncfg_if! {\n    if #[cfg(all(target_feature = \"sse2\", any(target_arch = \"x86\", target_arch =\n    \"x86_64\"), not(miri),))] { mod sse2; use sse2 as imp; } else if #[cfg(all(target_arch\n    = \"aarch64\", target_feature = \"neon\", target_endian = \"little\", not(miri),))] { mod\n    neon; use neon as imp; } else { mod generic; use generic as imp; }\n}\npub(crate) struct RawIterRange<T> {\n    current_group: BitMaskIter,\n    data: Bucket<T>,\n    next_ctrl: *const u8,\n    end: *const u8,\n}\n#[derive(Copy, Clone)]\npub(crate) struct BitMaskIter(pub(crate) BitMask);\n#[derive(Copy, Clone)]\npub(crate) struct BitMask(pub(crate) BitMaskWord);\npub struct Bucket<T> {\n    ptr: NonNull<T>,\n}\n#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n#[repr(transparent)]\npub(crate) struct Tag(u8);\nimpl<T> RawIterRange<T> {\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn new(ctrl: *const u8, data: Bucket<T>, len: usize) -> Self {\n        debug_assert_ne!(len, 0);\n        debug_assert_eq!(ctrl as usize % Group::WIDTH, 0);\n        let end = ctrl.add(len);\n        let current_group = Group::load_aligned(ctrl.cast()).match_full();\n        let next_ctrl = ctrl.add(Group::WIDTH);\n        Self {\n            current_group: current_group.into_iter(),\n            data,\n            next_ctrl,\n            end,\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    #[cfg(feature = \"rayon\")]\n    pub(crate) fn split(mut self) -> (Self, Option<RawIterRange<T>>) {\n        unsafe {\n            if self.end <= self.next_ctrl {\n                (self, None)\n            } else {\n                let len = offset_from(self.end, self.next_ctrl);\n                debug_assert_eq!(len % Group::WIDTH, 0);\n                let mid = (len / 2) & !(Group::WIDTH - 1);\n                let tail = Self::new(\n                    self.next_ctrl.add(mid),\n                    self.data.next_n(Group::WIDTH).next_n(mid),\n                    len - mid,\n                );\n                debug_assert_eq!(\n                    self.data.next_n(Group::WIDTH).next_n(mid).ptr, tail.data.ptr\n                );\n                debug_assert_eq!(self.end, tail.end);\n                self.end = self.next_ctrl.add(mid);\n                debug_assert_eq!(self.end.add(Group::WIDTH), tail.next_ctrl);\n                (self, Some(tail))\n            }\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn next_impl<const DO_CHECK_PTR_RANGE: bool>(&mut self) -> Option<Bucket<T>> {}\n    #[allow(clippy::while_let_on_iterator)]\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    unsafe fn fold_impl<F, B>(mut self, mut n: usize, mut acc: B, mut f: F) -> B\n    where\n        F: FnMut(B, Bucket<T>) -> B,\n    {\n        loop {\n            while let Some(index) = self.current_group.next() {\n                debug_assert!(n != 0);\n                let bucket = self.data.next_n(index);\n                acc = f(acc, bucket);\n                n -= 1;\n            }\n            if n == 0 {\n                return acc;\n            }\n            self.current_group = Group::load_aligned(self.next_ctrl.cast())\n                .match_full()\n                .into_iter();\n            self.data = self.data.next_n(Group::WIDTH);\n            self.next_ctrl = self.next_ctrl.add(Group::WIDTH);\n        }\n    }\n}\nimpl Iterator for BitMaskIter {\n    type Item = usize;\n    #[inline]\n    fn next(&mut self) -> Option<usize> {\n        let bit = self.0.lowest_set_bit()?;\n        self.0 = self.0.remove_lowest_bit();\n        Some(bit)\n    }\n}\nimpl IntoIterator for BitMask {\n    type Item = usize;\n    type IntoIter = BitMaskIter;\n    #[inline]\n    fn into_iter(self) -> BitMaskIter {\n        BitMaskIter(BitMask(self.0 & BITMASK_ITER_MASK))\n    }\n}\nimpl<T> Bucket<T> {\n    #[inline]\n    unsafe fn from_base_index(base: NonNull<T>, index: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(index + 1)\n        } else {\n            base.as_ptr().sub(index)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[inline]\n    unsafe fn to_base_index(&self, base: NonNull<T>) -> usize {}\n    #[inline]\n    pub fn as_ptr(&self) -> *mut T {}\n    #[inline]\n    fn as_non_null(&self) -> NonNull<T> {}\n    #[inline]\n    unsafe fn next_n(&self, offset: usize) -> Self {\n        let ptr = if T::IS_ZERO_SIZED {\n            invalid_mut(self.ptr.as_ptr() as usize + offset)\n        } else {\n            self.ptr.as_ptr().sub(offset)\n        };\n        Self {\n            ptr: NonNull::new_unchecked(ptr),\n        }\n    }\n    #[cfg_attr(feature = \"inline-more\", inline)]\n    pub(crate) unsafe fn drop(&self) {}\n    #[inline]\n    pub(crate) unsafe fn read(&self) -> T {}\n    #[inline]\n    pub(crate) unsafe fn write(&self, val: T) {}\n    #[inline]\n    pub unsafe fn as_ref<'a>(&self) -> &'a T {}\n    #[inline]\n    pub unsafe fn as_mut<'a>(&self) -> &'a mut T {}\n}\n\nThe function to be tested is presented as follows:\n/// Folds every element into an accumulator by applying an operation,\n/// returning the final result.\n///\n/// `fold_impl()` takes three arguments: the number of items remaining in\n/// the iterator, an initial value, and a closure with two arguments: an\n/// 'accumulator', and an element. The closure returns the value that the\n/// accumulator should have for the next iteration.\n///\n/// The initial value is the value the accumulator will have on the first call.\n///\n/// After applying this closure to every element of the iterator, `fold_impl()`\n/// returns the accumulator.\n///\n/// # Safety\n///\n/// If any of the following conditions are violated, the result is\n/// [`Undefined Behavior`]:\n///\n/// * The [`RawTableInner`] / [`RawTable`] must be alive and not moved,\n///   i.e. table outlives the `RawIterRange`;\n///\n/// * The provided `n` value must match the actual number of items\n///   in the table.\n///\n/// [`Undefined Behavior`]: https://doc.rust-lang.org/reference/behavior-considered-undefined.html\nunsafe fn fold_impl<F, B>(mut self, mut n: usize, mut acc: B, mut f: F) -> B\nwhere\n    F: FnMut(B, Bucket<T>) -> B,\n{\n    loop {\n        while let Some(index) = self.current_group.next() {\n            // The returned `index` will always be in the range `0..Group::WIDTH`,\n            // so that calling `self.data.next_n(index)` is safe (see detailed explanation below).\n            debug_assert!(n != 0);\n            let bucket = self.data.next_n(index);\n            acc = f(acc, bucket);\n            n -= 1;\n        }\n\n        if n == 0 {\n            return acc;\n        }\n\n        // SAFETY: The caller of this function ensures that:\n        //\n        // 1. The provided `n` value matches the actual number of items in the table;\n        // 2. The table is alive and did not moved.\n        //\n        // Taking the above into account, we always stay within the bounds, because:\n        //\n        // 1. For tables smaller than the group width (self.buckets() <= Group::WIDTH),\n        //    we will never end up in the given branch, since we should have already\n        //    yielded all the elements of the table.\n        //\n        // 2. For tables larger than the group width. The number of buckets is a\n        //    power of two (2 ^ n), Group::WIDTH is also power of two (2 ^ k). Since\n        //    `(2 ^ n) > (2 ^ k)`, than `(2 ^ n) % (2 ^ k) = 0`. As we start from the\n        //    start of the array of control bytes, and never try to iterate after\n        //    getting all the elements, the last `self.current_group` will read bytes\n        //    from the `self.buckets() - Group::WIDTH` index.  We know also that\n        //    `self.current_group.next()` will always return indices within the range\n        //    `0..Group::WIDTH`.\n        //\n        //    Knowing all of the above and taking into account that we are synchronizing\n        //    the `self.data` index with the index we used to read the `self.current_group`,\n        //    the subsequent `self.data.next_n(index)` will always return a bucket with\n        //    an index number less than `self.buckets()`.\n        //\n        //    The last `self.next_ctrl`, whose index would be `self.buckets()`, will never\n        //    actually be read, since we should have already yielded all the elements of\n        //    the table.\n        self.current_group = Group::load_aligned(self.next_ctrl.cast())\n            .match_full()\n            .into_iter();\n        self.data = self.data.next_n(Group::WIDTH);\n        self.next_ctrl = self.next_ctrl.add(Group::WIDTH);\n    }\n}\n",
  "depend_pt": ""
}