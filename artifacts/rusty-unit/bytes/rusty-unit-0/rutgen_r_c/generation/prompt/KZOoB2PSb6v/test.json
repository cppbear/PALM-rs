{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes_mut.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop, MaybeUninit};\nuse core::ops::{Deref, DerefMut};\nuse core::ptr::{self, NonNull};\nuse core::{cmp, fmt, hash, isize, slice, usize};\nuse alloc::{\n    borrow::{Borrow, BorrowMut},\n    boxed::Box, string::String, vec, vec::Vec,\n};\nuse crate::buf::{IntoIter, UninitSlice};\nuse crate::bytes::Vtable;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BufMut, Bytes, TryGetError};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_v_clone,\n    into_vec: shared_v_to_vec,\n    into_mut: shared_v_to_mut,\n    is_unique: shared_v_is_unique,\n    drop: shared_v_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nconst MAX_ORIGINAL_CAPACITY_WIDTH: usize = 17;\nconst MIN_ORIGINAL_CAPACITY_WIDTH: usize = 10;\nconst ORIGINAL_CAPACITY_MASK: usize = 0b11100;\nconst ORIGINAL_CAPACITY_OFFSET: usize = 2;\nconst VEC_POS_OFFSET: usize = 5;\nconst MAX_VEC_POS: usize = usize::MAX >> VEC_POS_OFFSET;\nconst NOT_VEC_POS_MASK: usize = 0b11111;\n#[cfg(target_pointer_width = \"64\")]\nconst PTR_WIDTH: usize = 64;\n#[cfg(target_pointer_width = \"32\")]\nconst PTR_WIDTH: usize = 32;\npub trait Buf {\n    fn remaining(&self) -> usize;\n    #[cfg_attr(docsrs, doc(alias = \"bytes\"))]\n    fn chunk(&self) -> &[u8];\n    #[cfg(feature = \"std\")]\n    #[cfg_attr(docsrs, doc(cfg(feature = \"std\")))]\n    fn chunks_vectored<'a>(&'a self, dst: &mut [IoSlice<'a>]) -> usize;\n    fn advance(&mut self, cnt: usize);\n    fn has_remaining(&self) -> bool;\n    fn copy_to_slice(&mut self, dst: &mut [u8]);\n    fn get_u8(&mut self) -> u8;\n    fn get_i8(&mut self) -> i8;\n    fn get_u16(&mut self) -> u16;\n    fn get_u16_le(&mut self) -> u16;\n    fn get_u16_ne(&mut self) -> u16;\n    fn get_i16(&mut self) -> i16;\n    fn get_i16_le(&mut self) -> i16;\n    fn get_i16_ne(&mut self) -> i16;\n    fn get_u32(&mut self) -> u32;\n    fn get_u32_le(&mut self) -> u32;\n    fn get_u32_ne(&mut self) -> u32;\n    fn get_i32(&mut self) -> i32;\n    fn get_i32_le(&mut self) -> i32;\n    fn get_i32_ne(&mut self) -> i32;\n    fn get_u64(&mut self) -> u64;\n    fn get_u64_le(&mut self) -> u64;\n    fn get_u64_ne(&mut self) -> u64;\n    fn get_i64(&mut self) -> i64;\n    fn get_i64_le(&mut self) -> i64;\n    fn get_i64_ne(&mut self) -> i64;\n    fn get_u128(&mut self) -> u128;\n    fn get_u128_le(&mut self) -> u128;\n    fn get_u128_ne(&mut self) -> u128;\n    fn get_i128(&mut self) -> i128;\n    fn get_i128_le(&mut self) -> i128;\n    fn get_i128_ne(&mut self) -> i128;\n    fn get_uint(&mut self, nbytes: usize) -> u64;\n    fn get_uint_le(&mut self, nbytes: usize) -> u64;\n    fn get_uint_ne(&mut self, nbytes: usize) -> u64;\n    fn get_int(&mut self, nbytes: usize) -> i64;\n    fn get_int_le(&mut self, nbytes: usize) -> i64;\n    fn get_int_ne(&mut self, nbytes: usize) -> i64;\n    fn get_f32(&mut self) -> f32;\n    fn get_f32_le(&mut self) -> f32;\n    fn get_f32_ne(&mut self) -> f32;\n    fn get_f64(&mut self) -> f64;\n    fn get_f64_le(&mut self) -> f64;\n    fn get_f64_ne(&mut self) -> f64;\n    fn try_copy_to_slice(&mut self, mut dst: &mut [u8]) -> Result<(), TryGetError>;\n    fn try_get_u8(&mut self) -> Result<u8, TryGetError>;\n    fn try_get_i8(&mut self) -> Result<i8, TryGetError>;\n    fn try_get_u16(&mut self) -> Result<u16, TryGetError>;\n    fn try_get_u16_le(&mut self) -> Result<u16, TryGetError>;\n    fn try_get_u16_ne(&mut self) -> Result<u16, TryGetError>;\n    fn try_get_i16(&mut self) -> Result<i16, TryGetError>;\n    fn try_get_i16_le(&mut self) -> Result<i16, TryGetError>;\n    fn try_get_i16_ne(&mut self) -> Result<i16, TryGetError>;\n    fn try_get_u32(&mut self) -> Result<u32, TryGetError>;\n    fn try_get_u32_le(&mut self) -> Result<u32, TryGetError>;\n    fn try_get_u32_ne(&mut self) -> Result<u32, TryGetError>;\n    fn try_get_i32(&mut self) -> Result<i32, TryGetError>;\n    fn try_get_i32_le(&mut self) -> Result<i32, TryGetError>;\n    fn try_get_i32_ne(&mut self) -> Result<i32, TryGetError>;\n    fn try_get_u64(&mut self) -> Result<u64, TryGetError>;\n    fn try_get_u64_le(&mut self) -> Result<u64, TryGetError>;\n    fn try_get_u64_ne(&mut self) -> Result<u64, TryGetError>;\n    fn try_get_i64(&mut self) -> Result<i64, TryGetError>;\n    fn try_get_i64_le(&mut self) -> Result<i64, TryGetError>;\n    fn try_get_i64_ne(&mut self) -> Result<i64, TryGetError>;\n    fn try_get_u128(&mut self) -> Result<u128, TryGetError>;\n    fn try_get_u128_le(&mut self) -> Result<u128, TryGetError>;\n    fn try_get_u128_ne(&mut self) -> Result<u128, TryGetError>;\n    fn try_get_i128(&mut self) -> Result<i128, TryGetError>;\n    fn try_get_i128_le(&mut self) -> Result<i128, TryGetError>;\n    fn try_get_i128_ne(&mut self) -> Result<i128, TryGetError>;\n    fn try_get_uint(&mut self, nbytes: usize) -> Result<u64, TryGetError>;\n    fn try_get_uint_le(&mut self, nbytes: usize) -> Result<u64, TryGetError>;\n    fn try_get_uint_ne(&mut self, nbytes: usize) -> Result<u64, TryGetError>;\n    fn try_get_int(&mut self, nbytes: usize) -> Result<i64, TryGetError>;\n    fn try_get_int_le(&mut self, nbytes: usize) -> Result<i64, TryGetError>;\n    fn try_get_int_ne(&mut self, nbytes: usize) -> Result<i64, TryGetError>;\n    fn try_get_f32(&mut self) -> Result<f32, TryGetError>;\n    fn try_get_f32_le(&mut self) -> Result<f32, TryGetError>;\n    fn try_get_f32_ne(&mut self) -> Result<f32, TryGetError>;\n    fn try_get_f64(&mut self) -> Result<f64, TryGetError>;\n    fn try_get_f64_le(&mut self) -> Result<f64, TryGetError>;\n    fn try_get_f64_ne(&mut self) -> Result<f64, TryGetError>;\n    fn copy_to_bytes(&mut self, len: usize) -> crate::Bytes;\n    fn take(self, limit: usize) -> Take<Self>\n    where\n        Self: Sized,\n    {\n        take::new(self, limit)\n    }\n    fn chain<U: Buf>(self, next: U) -> Chain<Self, U>\n    where\n        Self: Sized,\n    {\n        Chain::new(self, next)\n    }\n    #[cfg(feature = \"std\")]\n    #[cfg_attr(docsrs, doc(cfg(feature = \"std\")))]\n    fn reader(self) -> Reader<Self>\n    where\n        Self: Sized,\n    {\n        reader::new(self)\n    }\n}\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\npub struct Bytes {\n    ptr: *const u8,\n    len: usize,\n    data: AtomicPtr<()>,\n    vtable: &'static Vtable,\n}\npub(crate) struct Vtable {\n    /// fn(data, ptr, len)\n    pub clone: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> Bytes,\n    /// fn(data, ptr, len)\n    ///\n    /// `into_*` consumes the `Bytes`, returning the respective value.\n    pub into_vec: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> Vec<u8>,\n    pub into_mut: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> BytesMut,\n    /// fn(data)\n    pub is_unique: unsafe fn(&AtomicPtr<()>) -> bool,\n    /// fn(data, ptr, len)\n    pub drop: unsafe fn(&mut AtomicPtr<()>, *const u8, usize),\n}\nstruct Shared {\n    vec: Vec<u8>,\n    original_capacity_repr: usize,\n    ref_count: AtomicUsize,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {}\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {}\n    #[inline]\n    pub fn freeze(self) -> Bytes {\n        let bytes = ManuallyDrop::new(self);\n        if bytes.kind() == KIND_VEC {\n            unsafe {\n                let off = bytes.get_vec_pos();\n                let vec = rebuild_vec(bytes.ptr.as_ptr(), bytes.len, bytes.cap, off);\n                let mut b: Bytes = vec.into();\n                b.advance(off);\n                b\n            }\n        } else {\n            debug_assert_eq!(bytes.kind(), KIND_ARC);\n            let ptr = bytes.ptr.as_ptr();\n            let len = bytes.len;\n            let data = AtomicPtr::new(bytes.data.cast());\n            unsafe { Bytes::with_vtable(ptr, len, data, &SHARED_VTABLE) }\n        }\n    }\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {}\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {}\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {}\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {}\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {}\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {\n        self.data as usize & KIND_MASK\n    }\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {}\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {\n        debug_assert_eq!(self.kind(), KIND_VEC);\n        self.data as usize >> VEC_POS_OFFSET\n    }\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {}\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\nimpl Bytes {\n    #[inline]\n    #[cfg(not(all(loom, test)))]\n    pub const fn new() -> Self {\n        const EMPTY: &[u8] = &[];\n        Bytes::from_static(EMPTY)\n    }\n    #[cfg(all(loom, test))]\n    pub fn new() -> Self {\n        const EMPTY: &[u8] = &[];\n        Bytes::from_static(EMPTY)\n    }\n    #[inline]\n    #[cfg(not(all(loom, test)))]\n    pub const fn from_static(bytes: &'static [u8]) -> Self {\n        Bytes {\n            ptr: bytes.as_ptr(),\n            len: bytes.len(),\n            data: AtomicPtr::new(ptr::null_mut()),\n            vtable: &STATIC_VTABLE,\n        }\n    }\n    #[cfg(all(loom, test))]\n    pub fn from_static(bytes: &'static [u8]) -> Self {\n        Bytes {\n            ptr: bytes.as_ptr(),\n            len: bytes.len(),\n            data: AtomicPtr::new(ptr::null_mut()),\n            vtable: &STATIC_VTABLE,\n        }\n    }\n    fn new_empty_with_ptr(ptr: *const u8) -> Self {\n        debug_assert!(! ptr.is_null());\n        let ptr = without_provenance(ptr as usize);\n        Bytes {\n            ptr,\n            len: 0,\n            data: AtomicPtr::new(ptr::null_mut()),\n            vtable: &STATIC_VTABLE,\n        }\n    }\n    pub fn from_owner<T>(owner: T) -> Self\n    where\n        T: AsRef<[u8]> + Send + 'static,\n    {\n        let owned = Box::into_raw(\n            Box::new(Owned {\n                lifetime: OwnedLifetime {\n                    ref_cnt: AtomicUsize::new(1),\n                    drop: owned_box_and_drop::<T>,\n                },\n                owner,\n            }),\n        );\n        let mut ret = Bytes {\n            ptr: NonNull::dangling().as_ptr(),\n            len: 0,\n            data: AtomicPtr::new(owned.cast()),\n            vtable: &OWNED_VTABLE,\n        };\n        let buf = unsafe { &*owned }.owner.as_ref();\n        ret.ptr = buf.as_ptr();\n        ret.len = buf.len();\n        ret\n    }\n    #[inline]\n    pub const fn len(&self) -> usize {}\n    #[inline]\n    pub const fn is_empty(&self) -> bool {}\n    pub fn is_unique(&self) -> bool {}\n    pub fn copy_from_slice(data: &[u8]) -> Self {\n        data.to_vec().into()\n    }\n    pub fn slice(&self, range: impl RangeBounds<usize>) -> Self {\n        use core::ops::Bound;\n        let len = self.len();\n        let begin = match range.start_bound() {\n            Bound::Included(&n) => n,\n            Bound::Excluded(&n) => n.checked_add(1).expect(\"out of range\"),\n            Bound::Unbounded => 0,\n        };\n        let end = match range.end_bound() {\n            Bound::Included(&n) => n.checked_add(1).expect(\"out of range\"),\n            Bound::Excluded(&n) => n,\n            Bound::Unbounded => len,\n        };\n        assert!(\n            begin <= end, \"range start must not be greater than end: {:?} <= {:?}\",\n            begin, end,\n        );\n        assert!(end <= len, \"range end out of bounds: {:?} <= {:?}\", end, len,);\n        if end == begin {\n            return Bytes::new();\n        }\n        let mut ret = self.clone();\n        ret.len = end - begin;\n        ret.ptr = unsafe { ret.ptr.add(begin) };\n        ret\n    }\n    pub fn slice_ref(&self, subset: &[u8]) -> Self {\n        if subset.is_empty() {\n            return Bytes::new();\n        }\n        let bytes_p = self.as_ptr() as usize;\n        let bytes_len = self.len();\n        let sub_p = subset.as_ptr() as usize;\n        let sub_len = subset.len();\n        assert!(\n            sub_p >= bytes_p,\n            \"subset pointer ({:p}) is smaller than self pointer ({:p})\", subset.as_ptr(),\n            self.as_ptr(),\n        );\n        assert!(\n            sub_p + sub_len <= bytes_p + bytes_len,\n            \"subset is out of bounds: self = ({:p}, {}), subset = ({:p}, {})\", self\n            .as_ptr(), bytes_len, subset.as_ptr(), sub_len,\n        );\n        let sub_offset = sub_p - bytes_p;\n        self.slice(sub_offset..(sub_offset + sub_len))\n    }\n    #[must_use = \"consider Bytes::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> Self {\n        if at == self.len() {\n            return Bytes::new_empty_with_ptr(self.ptr.wrapping_add(at));\n        }\n        if at == 0 {\n            return mem::replace(self, Bytes::new_empty_with_ptr(self.ptr));\n        }\n        assert!(\n            at <= self.len(), \"split_off out of bounds: {:?} <= {:?}\", at, self.len(),\n        );\n        let mut ret = self.clone();\n        self.len = at;\n        unsafe { ret.inc_start(at) };\n        ret\n    }\n    #[must_use = \"consider Bytes::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> Self {\n        if at == self.len() {\n            let end_ptr = self.ptr.wrapping_add(at);\n            return mem::replace(self, Bytes::new_empty_with_ptr(end_ptr));\n        }\n        if at == 0 {\n            return Bytes::new_empty_with_ptr(self.ptr);\n        }\n        assert!(\n            at <= self.len(), \"split_to out of bounds: {:?} <= {:?}\", at, self.len(),\n        );\n        let mut ret = self.clone();\n        unsafe { self.inc_start(at) };\n        ret.len = at;\n        ret\n    }\n    #[inline]\n    pub fn truncate(&mut self, len: usize) {}\n    #[inline]\n    pub fn clear(&mut self) {}\n    pub fn try_into_mut(self) -> Result<BytesMut, Bytes> {}\n    #[inline]\n    pub(crate) unsafe fn with_vtable(\n        ptr: *const u8,\n        len: usize,\n        data: AtomicPtr<()>,\n        vtable: &'static Vtable,\n    ) -> Bytes {\n        Bytes { ptr, len, data, vtable }\n    }\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    unsafe fn inc_start(&mut self, by: usize) {}\n}\nunsafe fn rebuild_vec(\n    ptr: *mut u8,\n    mut len: usize,\n    mut cap: usize,\n    off: usize,\n) -> Vec<u8> {\n    let ptr = ptr.sub(off);\n    len += off;\n    cap += off;\n    Vec::from_raw_parts(ptr, len, cap)\n}\n\nThe function to be tested is presented as follows:\n/// Converts `self` into an immutable `Bytes`.\n///\n/// The conversion is zero cost and is used to indicate that the slice\n/// referenced by the handle will no longer be mutated. Once the conversion\n/// is done, the handle can be cloned and shared across threads.\n///\n/// # Examples\n///\n/// ```\n/// use bytes::{BytesMut, BufMut};\n/// use std::thread;\n///\n/// let mut b = BytesMut::with_capacity(64);\n/// b.put(&b\"hello world\"[..]);\n/// let b1 = b.freeze();\n/// let b2 = b1.clone();\n///\n/// let th = thread::spawn(move || {\n///     assert_eq!(&b1[..], b\"hello world\");\n/// });\n///\n/// assert_eq!(&b2[..], b\"hello world\");\n/// th.join().unwrap();\n/// ```\npub fn freeze(self) -> Bytes {\n    let bytes = ManuallyDrop::new(self);\n    if bytes.kind() == KIND_VEC {\n        // Just re-use `Bytes` internal Vec vtable\n        unsafe {\n            let off = bytes.get_vec_pos();\n            let vec = rebuild_vec(bytes.ptr.as_ptr(), bytes.len, bytes.cap, off);\n            let mut b: Bytes = vec.into();\n            b.advance(off);\n            b\n        }\n    } else {\n        debug_assert_eq!(bytes.kind(), KIND_ARC);\n\n        let ptr = bytes.ptr.as_ptr();\n        let len = bytes.len;\n        let data = AtomicPtr::new(bytes.data.cast());\n        unsafe { Bytes::with_vtable(ptr, len, data, &SHARED_VTABLE) }\n    }\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}