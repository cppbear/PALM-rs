{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop};\nuse core::ops::{Deref, RangeBounds};\nuse core::ptr::NonNull;\nuse core::{cmp, fmt, hash, ptr, slice, usize};\nuse alloc::{\n    alloc::{dealloc, Layout},\n    borrow::Borrow, boxed::Box, string::String, vec::Vec,\n};\nuse crate::buf::IntoIter;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BytesMut};\nstatic OWNED_VTABLE: Vtable = Vtable {\n    clone: owned_clone,\n    into_vec: owned_to_vec,\n    into_mut: owned_to_mut,\n    is_unique: owned_is_unique,\n    drop: owned_drop,\n};\nstatic PROMOTABLE_EVEN_VTABLE: Vtable = Vtable {\n    clone: promotable_even_clone,\n    into_vec: promotable_even_to_vec,\n    into_mut: promotable_even_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_even_drop,\n};\nstatic PROMOTABLE_ODD_VTABLE: Vtable = Vtable {\n    clone: promotable_odd_clone,\n    into_vec: promotable_odd_to_vec,\n    into_mut: promotable_odd_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_odd_drop,\n};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_clone,\n    into_vec: shared_to_vec,\n    into_mut: shared_to_mut,\n    is_unique: shared_is_unique,\n    drop: shared_drop,\n};\nconst STATIC_VTABLE: Vtable = Vtable {\n    clone: static_clone,\n    into_vec: static_to_vec,\n    into_mut: static_to_mut,\n    is_unique: static_is_unique,\n    drop: static_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {}\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {}\n    #[inline]\n    pub fn freeze(self) -> Bytes {}\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {}\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {}\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {}\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {\n        let mut vec = ManuallyDrop::new(vec);\n        let ptr = vptr(vec.as_mut_ptr());\n        let len = vec.len();\n        let cap = vec.capacity();\n        let original_capacity_repr = original_capacity_to_repr(cap);\n        let data = (original_capacity_repr << ORIGINAL_CAPACITY_OFFSET) | KIND_VEC;\n        BytesMut {\n            ptr,\n            len,\n            cap,\n            data: invalid_ptr(data),\n        }\n    }\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {\n        if count == 0 {\n            return;\n        }\n        debug_assert!(count <= self.cap, \"internal: set_start out of bounds\");\n        let kind = self.kind();\n        if kind == KIND_VEC {\n            let pos = self.get_vec_pos() + count;\n            if pos <= MAX_VEC_POS {\n                self.set_vec_pos(pos);\n            } else {\n                self.promote_to_shared(1);\n            }\n        }\n        self.ptr = vptr(self.ptr.as_ptr().add(count));\n        self.len = self.len.checked_sub(count).unwrap_or(0);\n        self.cap -= count;\n    }\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {}\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {}\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {}\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {}\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\nunsafe fn promotable_to_mut(\n    data: &AtomicPtr<()>,\n    ptr: *const u8,\n    len: usize,\n    f: fn(*mut ()) -> *mut u8,\n) -> BytesMut {\n    let shared = data.load(Ordering::Acquire);\n    let kind = shared as usize & KIND_MASK;\n    if kind == KIND_ARC {\n        shared_to_mut_impl(shared.cast(), ptr, len)\n    } else {\n        debug_assert_eq!(kind, KIND_VEC);\n        let buf = f(shared);\n        let off = offset_from(ptr, buf);\n        let cap = off + len;\n        let v = Vec::from_raw_parts(buf, cap, cap);\n        let mut b = BytesMut::from_vec(v);\n        b.advance_unchecked(off);\n        b\n    }\n}\n#[inline]\nfn offset_from(dst: *const u8, original: *const u8) -> usize {\n    dst as usize - original as usize\n}\nunsafe fn shared_to_mut_impl(\n    shared: *mut Shared,\n    ptr: *const u8,\n    len: usize,\n) -> BytesMut {\n    if (*shared).ref_cnt.load(Ordering::Acquire) == 1 {\n        let shared = *Box::from_raw(shared);\n        let shared = ManuallyDrop::new(shared);\n        let buf = shared.buf;\n        let cap = shared.cap;\n        let off = offset_from(ptr, buf);\n        let v = Vec::from_raw_parts(buf, len + off, cap);\n        let mut b = BytesMut::from_vec(v);\n        b.advance_unchecked(off);\n        b\n    } else {\n        let v = slice::from_raw_parts(ptr, len).to_vec();\n        release_shared(shared);\n        BytesMut::from_vec(v)\n    }\n}\n\nThe function to be tested is presented as follows:\nunsafe fn promotable_to_mut(\n    data: &AtomicPtr<()>,\n    ptr: *const u8,\n    len: usize,\n    f: fn(*mut ()) -> *mut u8,\n) -> BytesMut {\n    let shared = data.load(Ordering::Acquire);\n    let kind = shared as usize & KIND_MASK;\n\n    if kind == KIND_ARC {\n        shared_to_mut_impl(shared.cast(), ptr, len)\n    } else {\n        // KIND_VEC is a view of an underlying buffer at a certain offset.\n        // The ptr + len always represents the end of that buffer.\n        // Before truncating it, it is first promoted to KIND_ARC.\n        // Thus, we can safely reconstruct a Vec from it without leaking memory.\n        debug_assert_eq!(kind, KIND_VEC);\n\n        let buf = f(shared);\n        let off = offset_from(ptr, buf);\n        let cap = off + len;\n        let v = Vec::from_raw_parts(buf, cap, cap);\n\n        let mut b = BytesMut::from_vec(v);\n        b.advance_unchecked(off);\n        b\n    }\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}