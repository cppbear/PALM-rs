{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "// src/bytes_mut.rs\n// crate name is bytes\nThe function to be tested is presented as follows:\nfn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {\n    let len = self.len();\n    let kind = self.kind();\n\n    if kind == KIND_VEC {\n        // If there's enough free space before the start of the buffer, then\n        // just copy the data backwards and reuse the already-allocated\n        // space.\n        //\n        // Otherwise, since backed by a vector, use `Vec::reserve`\n        //\n        // We need to make sure that this optimization does not kill the\n        // amortized runtimes of BytesMut's operations.\n        unsafe {\n            let off = self.get_vec_pos();\n\n            // Only reuse space if we can satisfy the requested additional space.\n            //\n            // Also check if the value of `off` suggests that enough bytes\n            // have been read to account for the overhead of shifting all\n            // the data (in an amortized analysis).\n            // Hence the condition `off >= self.len()`.\n            //\n            // This condition also already implies that the buffer is going\n            // to be (at least) half-empty in the end; so we do not break\n            // the (amortized) runtime with future resizes of the underlying\n            // `Vec`.\n            //\n            // [For more details check issue #524, and PR #525.]\n            if self.capacity() - self.len() + off >= additional && off >= self.len() {\n                // There's enough space, and it's not too much overhead:\n                // reuse the space!\n                //\n                // Just move the pointer back to the start after copying\n                // data back.\n                let base_ptr = self.ptr.as_ptr().sub(off);\n                // Since `off >= self.len()`, the two regions don't overlap.\n                ptr::copy_nonoverlapping(self.ptr.as_ptr(), base_ptr, self.len);\n                self.ptr = vptr(base_ptr);\n                self.set_vec_pos(0);\n\n                // Length stays constant, but since we moved backwards we\n                // can gain capacity back.\n                self.cap += off;\n            } else {\n                if !allocate {\n                    return false;\n                }\n                // Not enough space, or reusing might be too much overhead:\n                // allocate more space!\n                let mut v =\n                    ManuallyDrop::new(rebuild_vec(self.ptr.as_ptr(), self.len, self.cap, off));\n                v.reserve(additional);\n\n                // Update the info\n                self.ptr = vptr(v.as_mut_ptr().add(off));\n                self.cap = v.capacity() - off;\n                debug_assert_eq!(self.len, v.len() - off);\n            }\n\n            return true;\n        }\n    }\n\n    debug_assert_eq!(kind, KIND_ARC);\n    let shared: *mut Shared = self.data;\n\n    // Reserving involves abandoning the currently shared buffer and\n    // allocating a new vector with the requested capacity.\n    //\n    // Compute the new capacity\n    let mut new_cap = match len.checked_add(additional) {\n        Some(new_cap) => new_cap,\n        None if !allocate => return false,\n        None => panic!(\"overflow\"),\n    };\n\n    unsafe {\n        // First, try to reclaim the buffer. This is possible if the current\n        // handle is the only outstanding handle pointing to the buffer.\n        if (*shared).is_unique() {\n            // This is the only handle to the buffer. It can be reclaimed.\n            // However, before doing the work of copying data, check to make\n            // sure that the vector has enough capacity.\n            let v = &mut (*shared).vec;\n\n            let v_capacity = v.capacity();\n            let ptr = v.as_mut_ptr();\n\n            let offset = offset_from(self.ptr.as_ptr(), ptr);\n\n            // Compare the condition in the `kind == KIND_VEC` case above\n            // for more details.\n            if v_capacity >= new_cap + offset {\n                self.cap = new_cap;\n                // no copy is necessary\n            } else if v_capacity >= new_cap && offset >= len {\n                // The capacity is sufficient, and copying is not too much\n                // overhead: reclaim the buffer!\n\n                // `offset >= len` means: no overlap\n                ptr::copy_nonoverlapping(self.ptr.as_ptr(), ptr, len);\n\n                self.ptr = vptr(ptr);\n                self.cap = v.capacity();\n            } else {\n                if !allocate {\n                    return false;\n                }\n                // calculate offset\n                let off = (self.ptr.as_ptr() as usize) - (v.as_ptr() as usize);\n\n                // new_cap is calculated in terms of `BytesMut`, not the underlying\n                // `Vec`, so it does not take the offset into account.\n                //\n                // Thus we have to manually add it here.\n                new_cap = new_cap.checked_add(off).expect(\"overflow\");\n\n                // The vector capacity is not sufficient. The reserve request is\n                // asking for more than the initial buffer capacity. Allocate more\n                // than requested if `new_cap` is not much bigger than the current\n                // capacity.\n                //\n                // There are some situations, using `reserve_exact` that the\n                // buffer capacity could be below `original_capacity`, so do a\n                // check.\n                let double = v.capacity().checked_shl(1).unwrap_or(new_cap);\n\n                new_cap = cmp::max(double, new_cap);\n\n                // No space - allocate more\n                //\n                // The length field of `Shared::vec` is not used by the `BytesMut`;\n                // instead we use the `len` field in the `BytesMut` itself. However,\n                // when calling `reserve`, it doesn't guarantee that data stored in\n                // the unused capacity of the vector is copied over to the new\n                // allocation, so we need to ensure that we don't have any data we\n                // care about in the unused capacity before calling `reserve`.\n                debug_assert!(off + len <= v.capacity());\n                v.set_len(off + len);\n                v.reserve(new_cap - v.len());\n\n                // Update the info\n                self.ptr = vptr(v.as_mut_ptr().add(off));\n                self.cap = v.capacity() - off;\n            }\n\n            return true;\n        }\n    }\n    if !allocate {\n        return false;\n    }\n\n    let original_capacity_repr = unsafe { (*shared).original_capacity_repr };\n    let original_capacity = original_capacity_from_repr(original_capacity_repr);\n\n    new_cap = cmp::max(new_cap, original_capacity);\n\n    // Create a new vector to store the data\n    let mut v = ManuallyDrop::new(Vec::with_capacity(new_cap));\n\n    // Copy the bytes\n    v.extend_from_slice(self.as_ref());\n\n    // Release the shared handle. This must be done *after* the bytes are\n    // copied.\n    unsafe { release_shared(shared) };\n\n    // Update self\n    let data = (original_capacity_repr << ORIGINAL_CAPACITY_OFFSET) | KIND_VEC;\n    self.data = invalid_ptr(data);\n    self.ptr = vptr(v.as_mut_ptr());\n    self.cap = v.capacity();\n    debug_assert_eq!(self.len, v.len());\n    return true;\n}\n",
  "depend_pt": ""
}