{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides. If the method under test uses generics, instantiate them with suitable types based on the context.\n3. Generate test code with minimal scope: avoid creating external structures or implementations. Instead, define any necessary helper structures or implementations directly within the test function when required.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context if exist.\n5. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n6. Create a minimal yet complete set of test functions, ensuring they adhere to all provided preconditions and cover boundary conditions.\n7. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop};\nuse core::ops::{Deref, RangeBounds};\nuse core::ptr::NonNull;\nuse core::{cmp, fmt, hash, ptr, slice, usize};\nuse alloc::{\n    alloc::{dealloc, Layout},\n    borrow::Borrow, boxed::Box, string::String, vec::Vec,\n};\nuse crate::buf::IntoIter;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BytesMut};\nstatic OWNED_VTABLE: Vtable = Vtable {\n    clone: owned_clone,\n    into_vec: owned_to_vec,\n    into_mut: owned_to_mut,\n    is_unique: owned_is_unique,\n    drop: owned_drop,\n};\nstatic PROMOTABLE_EVEN_VTABLE: Vtable = Vtable {\n    clone: promotable_even_clone,\n    into_vec: promotable_even_to_vec,\n    into_mut: promotable_even_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_even_drop,\n};\nstatic PROMOTABLE_ODD_VTABLE: Vtable = Vtable {\n    clone: promotable_odd_clone,\n    into_vec: promotable_odd_to_vec,\n    into_mut: promotable_odd_to_mut,\n    is_unique: promotable_is_unique,\n    drop: promotable_odd_drop,\n};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_clone,\n    into_vec: shared_to_vec,\n    into_mut: shared_to_mut,\n    is_unique: shared_is_unique,\n    drop: shared_drop,\n};\nconst STATIC_VTABLE: Vtable = Vtable {\n    clone: static_clone,\n    into_vec: static_to_vec,\n    into_mut: static_to_mut,\n    is_unique: static_is_unique,\n    drop: static_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\npub(crate) struct Vtable {\n    /// fn(data, ptr, len)\n    pub clone: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> Bytes,\n    /// fn(data, ptr, len)\n    ///\n    /// `into_*` consumes the `Bytes`, returning the respective value.\n    pub into_vec: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> Vec<u8>,\n    pub into_mut: unsafe fn(&AtomicPtr<()>, *const u8, usize) -> BytesMut,\n    /// fn(data)\n    pub is_unique: unsafe fn(&AtomicPtr<()>) -> bool,\n    /// fn(data, ptr, len)\n    pub drop: unsafe fn(&mut AtomicPtr<()>, *const u8, usize),\n}\npub struct Bytes {\n    ptr: *const u8,\n    len: usize,\n    data: AtomicPtr<()>,\n    vtable: &'static Vtable,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\n#[cold]\nunsafe fn shallow_clone_vec(\n    atom: &AtomicPtr<()>,\n    ptr: *const (),\n    buf: *mut u8,\n    offset: *const u8,\n    len: usize,\n) -> Bytes {\n    let shared = Box::new(Shared {\n        buf,\n        cap: offset_from(offset, buf) + len,\n        ref_cnt: AtomicUsize::new(2),\n    });\n    let shared = Box::into_raw(shared);\n    debug_assert!(\n        0 == (shared as usize & KIND_MASK),\n        \"internal: Box<Shared> should have an aligned pointer\",\n    );\n    match atom\n        .compare_exchange(ptr as _, shared as _, Ordering::AcqRel, Ordering::Acquire)\n    {\n        Ok(actual) => {\n            debug_assert!(actual as usize == ptr as usize);\n            Bytes {\n                ptr: offset,\n                len,\n                data: AtomicPtr::new(shared as _),\n                vtable: &SHARED_VTABLE,\n            }\n        }\n        Err(actual) => {\n            let shared = Box::from_raw(shared);\n            mem::forget(*shared);\n            shallow_clone_arc(actual as _, offset, len)\n        }\n    }\n}\n#[inline]\nfn offset_from(dst: *const u8, original: *const u8) -> usize {\n    dst as usize - original as usize\n}\nunsafe fn shallow_clone_arc(shared: *mut Shared, ptr: *const u8, len: usize) -> Bytes {\n    let old_size = (*shared).ref_cnt.fetch_add(1, Ordering::Relaxed);\n    if old_size > usize::MAX >> 1 {\n        crate::abort();\n    }\n    Bytes {\n        ptr,\n        len,\n        data: AtomicPtr::new(shared as _),\n        vtable: &SHARED_VTABLE,\n    }\n}\n\nThe function to be tested is presented as follows:\nunsafe fn shallow_clone_vec(\n    atom: &AtomicPtr<()>,\n    ptr: *const (),\n    buf: *mut u8,\n    offset: *const u8,\n    len: usize,\n) -> Bytes {\n    // If the buffer is still tracked in a `Vec<u8>`. It is time to\n    // promote the vec to an `Arc`. This could potentially be called\n    // concurrently, so some care must be taken.\n\n    // First, allocate a new `Shared` instance containing the\n    // `Vec` fields. It's important to note that `ptr`, `len`,\n    // and `cap` cannot be mutated without having `&mut self`.\n    // This means that these fields will not be concurrently\n    // updated and since the buffer hasn't been promoted to an\n    // `Arc`, those three fields still are the components of the\n    // vector.\n    let shared = Box::new(Shared {\n        buf,\n        cap: offset_from(offset, buf) + len,\n        // Initialize refcount to 2. One for this reference, and one\n        // for the new clone that will be returned from\n        // `shallow_clone`.\n        ref_cnt: AtomicUsize::new(2),\n    });\n\n    let shared = Box::into_raw(shared);\n\n    // The pointer should be aligned, so this assert should\n    // always succeed.\n    debug_assert!(\n        0 == (shared as usize & KIND_MASK),\n        \"internal: Box<Shared> should have an aligned pointer\",\n    );\n\n    // Try compare & swapping the pointer into the `arc` field.\n    // `Release` is used synchronize with other threads that\n    // will load the `arc` field.\n    //\n    // If the `compare_exchange` fails, then the thread lost the\n    // race to promote the buffer to shared. The `Acquire`\n    // ordering will synchronize with the `compare_exchange`\n    // that happened in the other thread and the `Shared`\n    // pointed to by `actual` will be visible.\n    match atom.compare_exchange(ptr as _, shared as _, Ordering::AcqRel, Ordering::Acquire) {\n        Ok(actual) => {\n            debug_assert!(actual as usize == ptr as usize);\n            // The upgrade was successful, the new handle can be\n            // returned.\n            Bytes {\n                ptr: offset,\n                len,\n                data: AtomicPtr::new(shared as _),\n                vtable: &SHARED_VTABLE,\n            }\n        }\n        Err(actual) => {\n            // The upgrade failed, a concurrent clone happened. Release\n            // the allocation that was made in this thread, it will not\n            // be needed.\n            let shared = Box::from_raw(shared);\n            mem::forget(*shared);\n\n            // Buffer already promoted to shared storage, so increment ref\n            // count.\n            shallow_clone_arc(actual as _, offset, len)\n        }\n    }\n}\n",
  "depend_pt": ""
}