{
  "system_pt": "As a software testing expert, please generate Rust test functions based on the following guidelines:\n1. Provide the code in plain text format, without explanations or Markdown.\n2. Omit test oracles and assertions; concentrate on generating test inputs and function calls of the focal function; do not use \"_\" for the return values of the focal function.\n3. Ensure the generated function is fully implemented and can be compiled and executed directly without any missing parts.\n4. Whenever possible, initialize the corresponding data structures using the initialization methods provided in the context.\n5. If the method under test belongs to a trait, construct appropriate structs within the test function, but avoid method overrides.\n6. If the method under test uses generics, instantiate them with suitable types based on the context.\n7. Define any necessary helper structures or implementations directly within the test function when required.\n8. Create a minimal yet comprehensive set of test functions, ensuring each test input satisfies all given constraints, with some explicitly covering edge scenarios.\n9. Do not create a test module, but include intrinsic attributes like #[test] or #[should_panic] where necessary.\n",
  "static_pt": "The context for the focal function is as follows:\n// src/bytes_mut.rs\n// crate name is bytes\nuse core::iter::FromIterator;\nuse core::mem::{self, ManuallyDrop, MaybeUninit};\nuse core::ops::{Deref, DerefMut};\nuse core::ptr::{self, NonNull};\nuse core::{cmp, fmt, hash, isize, slice, usize};\nuse alloc::{\n    borrow::{Borrow, BorrowMut},\n    boxed::Box, string::String, vec, vec::Vec,\n};\nuse crate::buf::{IntoIter, UninitSlice};\nuse crate::bytes::Vtable;\n#[allow(unused)]\nuse crate::loom::sync::atomic::AtomicMut;\nuse crate::loom::sync::atomic::{AtomicPtr, AtomicUsize, Ordering};\nuse crate::{offset_from, Buf, BufMut, Bytes, TryGetError};\nstatic SHARED_VTABLE: Vtable = Vtable {\n    clone: shared_v_clone,\n    into_vec: shared_v_to_vec,\n    into_mut: shared_v_to_mut,\n    is_unique: shared_v_is_unique,\n    drop: shared_v_drop,\n};\nconst _: [(); 0 - mem::align_of::<Shared>() % 2] = [];\nconst KIND_ARC: usize = 0b0;\nconst KIND_VEC: usize = 0b1;\nconst KIND_MASK: usize = 0b1;\nconst MAX_ORIGINAL_CAPACITY_WIDTH: usize = 17;\nconst MIN_ORIGINAL_CAPACITY_WIDTH: usize = 10;\nconst ORIGINAL_CAPACITY_MASK: usize = 0b11100;\nconst ORIGINAL_CAPACITY_OFFSET: usize = 2;\nconst VEC_POS_OFFSET: usize = 5;\nconst MAX_VEC_POS: usize = usize::MAX >> VEC_POS_OFFSET;\nconst NOT_VEC_POS_MASK: usize = 0b11111;\n#[cfg(target_pointer_width = \"64\")]\nconst PTR_WIDTH: usize = 64;\n#[cfg(target_pointer_width = \"32\")]\nconst PTR_WIDTH: usize = 32;\npub struct BytesMut {\n    ptr: NonNull<u8>,\n    len: usize,\n    cap: usize,\n    data: *mut Shared,\n}\nstruct Shared {\n    buf: *mut u8,\n    cap: usize,\n    ref_cnt: AtomicUsize,\n}\nstruct Shared {\n    vec: Vec<u8>,\n    original_capacity_repr: usize,\n    ref_count: AtomicUsize,\n}\nimpl BytesMut {\n    #[inline]\n    pub fn with_capacity(capacity: usize) -> BytesMut {}\n    #[inline]\n    pub fn new() -> BytesMut {}\n    #[inline]\n    pub fn len(&self) -> usize {\n        self.len\n    }\n    #[inline]\n    pub fn is_empty(&self) -> bool {}\n    #[inline]\n    pub fn capacity(&self) -> usize {\n        self.cap\n    }\n    #[inline]\n    pub fn freeze(self) -> Bytes {}\n    pub fn zeroed(len: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::truncate if you don't need the other half\"]\n    pub fn split_off(&mut self, at: usize) -> BytesMut {}\n    #[must_use = \"consider BytesMut::clear if you don't need the other half\"]\n    pub fn split(&mut self) -> BytesMut {}\n    #[must_use = \"consider BytesMut::advance if you don't need the other half\"]\n    pub fn split_to(&mut self, at: usize) -> BytesMut {}\n    pub fn truncate(&mut self, len: usize) {}\n    pub fn clear(&mut self) {}\n    pub fn resize(&mut self, new_len: usize, value: u8) {}\n    #[inline]\n    pub unsafe fn set_len(&mut self, len: usize) {}\n    #[inline]\n    pub fn reserve(&mut self, additional: usize) {}\n    fn reserve_inner(&mut self, additional: usize, allocate: bool) -> bool {\n        let len = self.len();\n        let kind = self.kind();\n        if kind == KIND_VEC {\n            unsafe {\n                let off = self.get_vec_pos();\n                if self.capacity() - self.len() + off >= additional && off >= self.len()\n                {\n                    let base_ptr = self.ptr.as_ptr().sub(off);\n                    ptr::copy_nonoverlapping(self.ptr.as_ptr(), base_ptr, self.len);\n                    self.ptr = vptr(base_ptr);\n                    self.set_vec_pos(0);\n                    self.cap += off;\n                } else {\n                    if !allocate {\n                        return false;\n                    }\n                    let mut v = ManuallyDrop::new(\n                        rebuild_vec(self.ptr.as_ptr(), self.len, self.cap, off),\n                    );\n                    v.reserve(additional);\n                    self.ptr = vptr(v.as_mut_ptr().add(off));\n                    self.cap = v.capacity() - off;\n                    debug_assert_eq!(self.len, v.len() - off);\n                }\n                return true;\n            }\n        }\n        debug_assert_eq!(kind, KIND_ARC);\n        let shared: *mut Shared = self.data;\n        let mut new_cap = match len.checked_add(additional) {\n            Some(new_cap) => new_cap,\n            None if !allocate => return false,\n            None => panic!(\"overflow\"),\n        };\n        unsafe {\n            if (*shared).is_unique() {\n                let v = &mut (*shared).vec;\n                let v_capacity = v.capacity();\n                let ptr = v.as_mut_ptr();\n                let offset = offset_from(self.ptr.as_ptr(), ptr);\n                if v_capacity >= new_cap + offset {\n                    self.cap = new_cap;\n                } else if v_capacity >= new_cap && offset >= len {\n                    ptr::copy_nonoverlapping(self.ptr.as_ptr(), ptr, len);\n                    self.ptr = vptr(ptr);\n                    self.cap = v.capacity();\n                } else {\n                    if !allocate {\n                        return false;\n                    }\n                    let off = (self.ptr.as_ptr() as usize) - (v.as_ptr() as usize);\n                    new_cap = new_cap.checked_add(off).expect(\"overflow\");\n                    let double = v.capacity().checked_shl(1).unwrap_or(new_cap);\n                    new_cap = cmp::max(double, new_cap);\n                    debug_assert!(off + len <= v.capacity());\n                    v.set_len(off + len);\n                    v.reserve(new_cap - v.len());\n                    self.ptr = vptr(v.as_mut_ptr().add(off));\n                    self.cap = v.capacity() - off;\n                }\n                return true;\n            }\n        }\n        if !allocate {\n            return false;\n        }\n        let original_capacity_repr = unsafe { (*shared).original_capacity_repr };\n        let original_capacity = original_capacity_from_repr(original_capacity_repr);\n        new_cap = cmp::max(new_cap, original_capacity);\n        let mut v = ManuallyDrop::new(Vec::with_capacity(new_cap));\n        v.extend_from_slice(self.as_ref());\n        unsafe { release_shared(shared) };\n        let data = (original_capacity_repr << ORIGINAL_CAPACITY_OFFSET) | KIND_VEC;\n        self.data = invalid_ptr(data);\n        self.ptr = vptr(v.as_mut_ptr());\n        self.cap = v.capacity();\n        debug_assert_eq!(self.len, v.len());\n        return true;\n    }\n    #[inline]\n    #[must_use = \"consider BytesMut::reserve if you need an infallible reservation\"]\n    pub fn try_reclaim(&mut self, additional: usize) -> bool {\n        let len = self.len();\n        let rem = self.capacity() - len;\n        if additional <= rem {\n            return true;\n        }\n        self.reserve_inner(additional, false)\n    }\n    #[inline]\n    pub fn extend_from_slice(&mut self, extend: &[u8]) {}\n    pub fn unsplit(&mut self, other: BytesMut) {}\n    #[inline]\n    pub(crate) fn from_vec(vec: Vec<u8>) -> BytesMut {}\n    #[inline]\n    fn as_slice(&self) -> &[u8] {}\n    #[inline]\n    fn as_slice_mut(&mut self) -> &mut [u8] {}\n    pub(crate) unsafe fn advance_unchecked(&mut self, count: usize) {}\n    fn try_unsplit(&mut self, other: BytesMut) -> Result<(), BytesMut> {}\n    #[inline]\n    fn kind(&self) -> usize {}\n    unsafe fn promote_to_shared(&mut self, ref_cnt: usize) {}\n    #[inline]\n    unsafe fn shallow_clone(&mut self) -> BytesMut {}\n    #[inline]\n    unsafe fn get_vec_pos(&self) -> usize {}\n    #[inline]\n    unsafe fn set_vec_pos(&mut self, pos: usize) {}\n    #[inline]\n    pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {}\n}\n\nThe function to be tested is presented as follows:\n/// Attempts to cheaply reclaim already allocated capacity for at least `additional` more\n/// bytes to be inserted into the given `BytesMut` and returns `true` if it succeeded.\n///\n/// `try_reclaim` behaves exactly like `reserve`, except that it never allocates new storage\n/// and returns a `bool` indicating whether it was successful in doing so:\n///\n/// `try_reclaim` returns false under these conditions:\n///  - The spare capacity left is less than `additional` bytes AND\n///  - The existing allocation cannot be reclaimed cheaply or it was less than\n///    `additional` bytes in size\n///\n/// Reclaiming the allocation cheaply is possible if the `BytesMut` has no outstanding\n/// references through other `BytesMut`s or `Bytes` which point to the same underlying\n/// storage.\n///\n/// # Examples\n///\n/// ```\n/// use bytes::BytesMut;\n///\n/// let mut buf = BytesMut::with_capacity(64);\n/// assert_eq!(true, buf.try_reclaim(64));\n/// assert_eq!(64, buf.capacity());\n///\n/// buf.extend_from_slice(b\"abcd\");\n/// let mut split = buf.split();\n/// assert_eq!(60, buf.capacity());\n/// assert_eq!(4, split.capacity());\n/// assert_eq!(false, split.try_reclaim(64));\n/// assert_eq!(false, buf.try_reclaim(64));\n/// // The split buffer is filled with \"abcd\"\n/// assert_eq!(false, split.try_reclaim(4));\n/// // buf is empty and has capacity for 60 bytes\n/// assert_eq!(true, buf.try_reclaim(60));\n///\n/// drop(buf);\n/// assert_eq!(false, split.try_reclaim(64));\n///\n/// split.clear();\n/// assert_eq!(4, split.capacity());\n/// assert_eq!(true, split.try_reclaim(64));\n/// assert_eq!(64, split.capacity());\n/// ```\npub fn try_reclaim(&mut self, additional: usize) -> bool {\n    let len = self.len();\n    let rem = self.capacity() - len;\n\n    if additional <= rem {\n        // The handle can already store at least `additional` more bytes, so\n        // there is no further work needed to be done.\n        return true;\n    }\n\n    self.reserve_inner(additional, false)\n}\nGiven the following constraints, potential panic-triggering statements, and expected return values/types (all extracted from the function under test).\nGenerate test inputs that maximize the function's runtime satisfaction of all constraints and expected outputs while considering panic conditions:\n",
  "depend_pt": ""
}